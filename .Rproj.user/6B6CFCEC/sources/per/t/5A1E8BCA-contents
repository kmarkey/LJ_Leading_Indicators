---
title: 'Leading Indicators for LJ Kirkland'
subtitle: 'First Report'
author: "Keaton Markey"
date:  "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document:
    latex_engine: xelatex
    toc: true
    number_sections: true
    toc_depth: 6
    highlight: tango
  html_document:
    toc: yes
urlcolor: blue
---

### Project Aim:

From the last 5 years of data, we are trying to find external indicators of a 
variety of business metrics within the Lee Johnson Auto Family's Kirkland location.
I will gather a time-dependent economic data and create a model that predicts business 
performance.

## Process:

### 1. Digest a Prepare LJ Data

### 2. Fetch and Prepare Indicators

### 3. Create 3 Different Models
  a) Elastic Net
  b) LS-SVM
  c) RNN

### 4. Figure Appendix

```{r, include=FALSE, echo = FALSE}
setwd("~/LocalRStudio/LJ_Leading_Indicators")
source("Tranform.R")
source("Digest.R")

# some important variables
max(features$date)
# generated on
today <- Sys.Date()

```

```{r theme setup, include = FALSE}
ljtheme <- function() {
  theme_minimal() +
    theme_update(base_size = 11, base_family = "", base_line_size = base_size/22,
}
theme_gppr <- function(){ 
    font <- "Georgia"   #assign font family up front
    
    theme_classic() %+replace%    #replace elements we want to change
    
    theme(
      
      #grid elements
      panel.grid.major = element_blank(),    #strip major gridlines
          #strip minor gridlines
      
      #since theme_minimal() already strips axis lines, 
      #we don't need to do that again
      
      #text elements
      plot.title = element_text(             #title
                   family = font,            #set font family
                   size = 20,                #set font size
                   face = 'bold',            #bold typeface
                   hjust = 0,                #left align
                   vjust = 2),               #raise slightly
      
      plot.subtitle = element_text(          #subtitle
                   family = font,            #font family
                   size = 14),               #font size
      
      plot.caption = element_text(           #caption
                   family = font,            #font family
                   size = 9,                 #font size
                   hjust = 1),               #right align
      
      axis.title = element_text(             #axis titles
                   family = font,            #font family
                   size = 10),               #font size
      
      axis.text = element_text(              #axis text
                   family = font,            #axis famuly
                   size = 9),                #font size
      
      axis.text.x = element_text(            #margin for axis text
                    margin=margin(5, b = 10))
      
      #since the legend often requires manual tweaking 
      #based on plot content, don't define it here
    )
}

```



## 1. Digestion

After receiving LJ data, all other data is fetched from either Quandl, a free market API, or Yahoo Finance for stock data.

Here's whats new for Lee Johnson

```{r LJ New}
today - 30
KDAt_nm <- KDAt %>% # new month
  dplyr::filter(date >= max(floor_date(date, "month"))) # most recent month

KDAt_nm %>%
  group_by(date) %>%
  mutate(ycount = n()) %>%
  ungroup() %>%
  ggplot() + geom_line(aes(x = date, y = ycount)) +
  theme_gppr()

KDAt_nm


lubridate::year(day_1_c$date)
ymd(stop_date)
lubridate::
KDAt_old

```

What do overall trends look like from last month? moving avg?
```{r}

```

### 2. Summary statistics

```{r}
summary(KDA1)
glimpse(KDA1)
```

#### We have some NA's, or empty spaces, in the data. This could be for a 
variety of reasons, someone didn't enter in a number, it got lost in translation, 
or just didn't exist in the first place. Before we deal with these missing values, 
let's see how much they affect our data. We really only care about NAs in numerical 
columns, because thats when they could affect the stats we will run later.

#### We can use logistic regression to tell us how much the NAs might influence
the data longitudinally. This is probably unnecessary but I'm going to do it anyway.
```{r}
summary(glm(is.na(Back_Gross_Profit) ~ Date, KDA1, family = 'binomial'))
summary(glm(is.na(Front_Gross_Profit) ~ Date, KDA1, family = 'binomial'))
summary(glm(is.na(Total_Gross_Profit) ~ Date, KDA1, family = 'binomial'))
summary(glm(is.na(Cash_Price) ~ Date, KDA1, family = 'binomial'))

```
#### Based on the models, we noticed a significant trend for all the metrics.
It shouldn't impact analysis too much because we didn't test numeric variables, 
but its something to keep in mind.

  Generally, we saw more missing Front Gross Profit values and Cash Price values
  at the end of the data set, and we saw more missing values for Back Gross Profit
  and Total Gross Profit at the beginning of the data set.
  
### 3. Plotting

#### Now, lets look at some quick plots of the data.
```{r}
ggplot(KDA1, aes(Date, Front_Gross_Profit)) + geom_point() + geom_smooth()
ggplot(KDA1, aes(Date, Back_Gross_Profit)) + geom_point() + geom_smooth()
ggplot(KDA1, aes(Date, Total_Gross_Profit)) + geom_point() + geom_smooth()
ggplot(KDA1, aes(Date, Cash_Price)) + geom_point() + geom_smooth()
```

#### 
```{r}
ggplot(KDA1, aes(x = Date)) + geom_smooth(aes(y = Front_Gross_Profit, col = "front"), se = F) +
  geom_smooth(aes(y = Back_Gross_Profit, col = "back"), se = F) + ylab("") +
  geom_smooth(aes(y = Total_Gross_Profit, col = "total"), se = F) +
  geom_smooth(aes(y = .04*Cash_Price, col = "cash"), se = F) 
```
#### If we hope to do anything with this, we will have to extract these trends. 
To make this easier, lets create a new df for each day.

### 4. Condensing
```{r}
Day<- as.data.frame(seq(as.Date("2016-01-01"), as.Date("2020-11-16"), by="days"))
colnames(Day)<-c("Date")
temp<- group_by(KDA1, Date) %>%
  mutate(sum.fgp = sum(Front_Gross_Profit, na.rm = TRUE)) %>%
  summarise(sum.fgp, .groups = 'drop') %>%
  unique()
Day<-left_join(Day, count(group_by(KDA1, Date)), by = "Date")
Day<-left_join(Day, temp, by = "Date")
temp<- group_by(KDA1, Date) %>%
  mutate(sum.bgp= sum(Back_Gross_Profit, na.rm = TRUE)) %>%
  summarise(sum.bgp, .groups = 'drop') %>%
  unique()
Day<-left_join(Day, temp, by = "Date")
temp<- group_by(KDA1, Date) %>%
  mutate(sum.tgp = sum(Total_Gross_Profit, na.rm = TRUE)) %>%
  summarise(sum.tgp, .groups = 'drop') %>%
  unique()
Day<-left_join(Day, temp, by = "Date")
temp<- group_by(KDA1, Date) %>%
  mutate(sum.cp = sum(Cash_Price, na.rm = TRUE)) %>%
  summarise(sum.cp, .groups = 'drop') %>%
  unique()
Day<-left_join(Day, temp, by = "Date")
```
#### There are 1782 days, with 55 NAs in the mix. Most of them are probably within
that stretch in April of 2019. We will fix them by imputation. First lets
take a look at this new df, now with the number of sales per day [n]. The missing
values will automatically be predicted using a simple linear model (loess).
```{r}
summary(Day)
Day %>%
ggplot(aes(x = Date)) + 
  geom_smooth(aes(y =.6*sum.tgp-600, color = "total gross"), 
              se = F, method = 'loess') +
  geom_smooth(aes(y = sum.bgp, color = "back gross"), 
              se = F, method = 'loess') + 
  geom_smooth(aes(y = .6*sum.fgp + 8700, color = "front gross"), 
              se = F, method = 'loess') +
  geom_smooth(aes(y = .15*(sum.cp-183000), color = "cash price"), 
              se = F, method = 'loess') +
  geom_smooth(aes(y = 1000*n+5000, color = "n sales"), 
              se = F, method = 'loess') + ylab("")
```
### 5. Imputation

#### I've chosen to impute the mssing values so we can get a complete df that we
can run through functions and expect a complete output. To do this, we can use a 
variety of methods. I've chosen to use predictive mean matching, which uses a
kind of regression that builds relationships around the mean of each variable.
Again, there are many ways to do this and this might not be the best one. First,
we train the algorithm 20 times [m = 20], and we get its best guess at what the 
missing values would be according to pmm. We can easily change the method later too.
```{r, include=FALSE}
m<-'pmm'
junk<-capture.output(imp <- mice(data = Day, m = 20, method = c("",m,m,m,m,m)))
Day.full<-complete(imp, 20)
#The completed df
Day.full %>%
ggplot(aes(x = Date)) + 
  geom_smooth(aes(y =.6*sum.tgp-600, color = "total gross"), 
              se = F, method = 'loess') +
  geom_smooth(aes(y = sum.bgp, color = "back gross"), 
              se = F, method = 'loess') + 
  geom_smooth(aes(y = .6*sum.fgp + 8700, color = "front gross"), 
              se = F, method = 'loess') +
  geom_smooth(aes(y = .15*(sum.cp-183000), color = "cash price"), 
              se = F, method = 'loess') +
  geom_smooth(aes(y = 1000*n+5000, color = "n sales"), 
              se = F, method = 'loess') + ylab("")
```
#### Now, since we have data for each day, we can do some timeries analysis for
all of our metrics. [freq = 365] because we have 365 observations per year.
Remember here, these values are taken from the sum of that value every day.
From, these plots, we really just need to pay attention to the trend.
```{r}
b<-decompose(ts(Day.full$sum.bgp, frequency =365))
plot(b) + title("Back Gross", line = -.5)
f<-decompose(ts(Day.full$sum.fgp, frequency =365))
plot(f)+ title("Front Gross", line = -.5)
t<-decompose(ts(Day.full$sum.tgp, frequency =365))
plot(t) + title("Total Gross", line = -.5)
c<-decompose(ts(Day.full$sum.cp, frequency =365))
plot(c) + title("Cash Price", line = -.5)
n<-decompose(ts(Day.full$n, frequency =365))
plot(n) + title("N Sold", line =  -.5)
```
#### Looks nice and informative. Now, lets make sure all our data is in the standard
style so we can easily compare it to other economic trends.
```{r}
#dummy for joining
time.seq<-seq.Date(from = as.Date("2016-01-01"), to = as.Date("2020-11-16"), by = 'day')
blank<-as.data.frame(time.seq)
colnames(blank)<-c("date")
## Then the final df
rogue<- cbind(as.data.frame(Day.full$Date), as.data.frame(b$trend),
              as.data.frame(f$trend),
              as.data.frame(t$trend), as.data.frame(c$trend), 
              as.data.frame(n$trend))
colnames(rogue)<-c("date", "backg", "frontg", "totalg", "cp", "n")
rogue<-left_join(blank, rogue)
#Finally, a very informative plot of our trends so far
plot(rogue)
```
#### Now, we are ready to test this df against some possible leading indicators.
  As I understand it, leading indicators will show the current trend some number 
  of days, weeks, or even months. So instead  of just joining two df by date against
  each other and grabbing the correlation from there, we'll want to modulate how far
  in the past (or the future) we offset one df. Ideally, we would check the correlation
  at a number of offset points, to  find the highest value we can get from the data.
  I need to design a function to compile each new df and put it into a table of 
  correlations with LJ data.

## Part 2.
```{r}







```









