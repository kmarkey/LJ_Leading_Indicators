cor(a$Close,b$t.s.fgp)
#GM follows LJ, so no luck there
#Cor just for fun
cor(GM$Adj.Close, rogue$t.s.tgp, method = 'pearson', use = 'pairwise')
b<-filter(rogue, date>= "2019-08-01")
cor(a$Close,b$t.s.fgp)
b<-filter(rogue, date>= "2018-08-01")
cor(a$Close,b$t.s.fgp)
cancor(a$Close,b$t.s.fgp)
#Not bad
#should write function to do this for me when I import data
a<-filter(GM, date>="2018-08-01")
b<-filter(rogue, date>= "2018-08-01")
cancor(a$Close,b$t.s.fgp)
cancor(a$Adj.Close,b$t.s.fgp)
#again a normalized plot
ggplot(rogue, aes(date, t.s.tgp, color = "LJ tgp")) + geom_line() +
geom_line(aes(GM$date, 150*GM$Adj.Close + 21000, color = "GM")) +
geom_line(aes(date, t.s.fgp, color = "LJ fgp")) +
geom_line(aes(date, t.s.bgp, color = "LJ bgp")) +
geom_line(aes(date, .05*t.s.cp, color = "LJ cp"))
cancor(a$Adj.Close,b$t.s.fgp, method = "spearman")
cancor(a$Adj.Close,b$t.s.fgp, method = "pearson")
cor(a$Adj.Close,b$t.s.fgp, method = "pearson")
#Not bad
#should write function to do this for me when I import data
a<-filter(GM, date<="2018-08-01")
b<-filter(rogue, date<= "2018-08-01")
cor(a$Adj.Close,b$t.s.fgp, method = "pearson")
cor(GM$Adj.Close,rogue$t.s.fgp, method = "pearson")
cor(GM$Adj.Close, rogue$t.s.fgp, method = "pearson")
#GM follows LJ, so no luck there
#Cor just for fun
cor(GM$Adj.Close, rogue$t.s.tgp, method = 'pearson', use = 'pairwise')
cor(GM$Adj.Close, rogue$t.s.fgp, method = "pearson", use = 'pairwise')
b<-filter(rogue, date<= "2019-08-01")
#Not bad
#should write function to do this for me when I import data
a<-filter(GM, date<="2019-08-01")
b<-filter(rogue, date<= "2019-08-01")
cor(GM$Adj.Close, rogue$t.s.fgp, method = "pearson", use = 'pairwise')
#Not bad
#should write function to do this for me when I import data
a<-filter(GM, date<="2019-08-01")
b<-filter(rogue, date<= "2019-08-01")
cor(GM$Adj.Close, rogue$t.s.fgp, method = "pearson", use = 'pairwise')
#Not bad
#should write function to do this for me when I import data
a<-filter(GM, date<="2020-08-01")
b<-filter(rogue, date<= "2020-08-01")
cor(GM$Adj.Close, rogue$t.s.fgp, method = "pearson", use = 'pairwise')
cor(GM$Adj.Close, rogue$t.s.fgp, method = "pearson", use = 'everything')
cor(GM$Adj.Close, rogue$t.s.fgp, method = "pearson", use = 'complete.obs')
cor(GM$Adj.Close, rogue$t.s.fgp, method = "pearson")
cor(GM$Adj.Close, rogue$t.s.fgp, method = "pearson", use = 'pairwise')
#Not bad
#should write function to do this for me when I import data
a<-filter(GM, date>="2020-08-01")
b<-filter(rogue, dat>= "2020-08-01")
b<-filter(rogue, date>= "2020-08-01")
cor(GM$Adj.Close, rogue$t.s.fgp, method = "pearson", use = 'pairwise')
#Not bad
#should write function to do this for me when I import data
a<-filter(GM, date>="2019-08-01")
b<-filter(rogue, date>= "2019-08-01")
cor(GM$Adj.Close, rogue$t.s.fgp, method = "pearson", use = 'pairwise')
#GM follows LJ, so no luck there
#Cor just for fun
cor(GM$Adj.Close, rogue$t.s.tgp, method = 'pearson', use = 'pairwise')
cor(GM$Adj.Close, rogue$t.s.tgp, method = "pearson", use = 'pairwise')
cor(a$Adj.Close, b$t.s.tgp, method = "pearson", use = 'pairwise')
#Not bad
#should write function to do this for me when I import data
a<-filter(GM, date>="2019-08-01")
b<-filter(rogue, date>= "2019-08-01")
cor(a$Adj.Close, b$t.s.tgp, method = "pearson", use = 'pairwise')
b<-filter(rogue, date>= "2018-08-01")
#Not bad
#should write function to do this for me when I import data
a<-filter(GM, date>="2018-08-01")
b<-filter(rogue, date>= "2018-08-01")
cor(a$Adj.Close, b$t.s.tgp, method = "pearson", use = 'pairwise')
#Not bad
#should write function to do this for me when I import data
a<-filter(GM, date>="2020-08-01")
b<-filter(rogue, date>= "2020-08-01")
cor(a$Adj.Close, b$t.s.tgp, method = "pearson", use = 'pairwise')
#Not bad
#should write function to do this for me when I import data
range(GM)
#Not bad
#should write function to do this for me when I import data
range(GM$Adj.Close)
#Not bad
#should write function to do this for me when I import data
range(na.omit(GM$date))
#Not bad
#should write function to do this for me when I import data
range(na.omit(GM$date))-12
knitr::opts_chunk$set(
cache = FALSE, # if TRUE knitr will cache results to reuse in future knits
fig.width = 6, # the width for plots created by code chunk
fig.height = 4.5, # the height for plots created by code chunk
fig.align = 'center', # how to align graphics. 'left', 'right', 'center'
dpi = 150,
results = 'asis', # knitr passes through results without reformatting
echo = TRUE, # if FALSE knitr won't display code in chunk above it's results
message = TRUE, # if FALSE knitr won't display messages generated by code
strip.white = TRUE, # if FALSE knitr won't remove white spaces at beg or end of code chunk
warning = FALSE) # if FALSE knitr won't display warning messages in the doc
#Finally, a very informative plot of our trends so far
plot(rogue)
summary(glm(is.na(Back_Gross_Profit) ~ Date, KDA1, family = 'binomial'))
summary(glm(is.na(Front_Gross_Profit) ~ Date, KDA1, family = 'binomial'))
summary(glm(is.na(Total_Gross_Profit) ~ Date, KDA1, family = 'binomial'))
summary(glm(is.na(Cash_Price) ~ Date, KDA1, family = 'binomial'))
summary(glm(is.na(Back_Gross_Profit) ~ Date + Total_Gross_Profit, KDA1, family = 'binomial'))
summary(glm(is.na(Back_Gross_Profit) ~ Date, KDA1, family = 'binomial'))
summary(glm(is.na(Total_Gross_Profit) ~ Date, KDA1, family = 'binomial'))
summary(glm(is.na(Total_Gross_Profit) ~ Date, KDA1, family = 'binomial'))
summary(glm(is.na(Cash_Price) ~ Date, KDA1, family = 'binomial'))
ggplot(KDA1, aes(Date, Front_Gross_Profit)) + geom_point() + geom_smooth()
ggplot(KDA1, aes(Date, Back_Gross_Profit)) + geom_point() + geom_smooth()
ggplot(KDA1, aes(Date, Total_Gross_Profit)) + geom_point() + geom_smooth()
ggplot(KDA1, aes(Date, Cash_Price)) + geom_point() + geom_smooth()
ggplot(KDA1, aes(x = Date)) + geom_smooth(aes(y = Front_Gross_Profit, col = "front"), se = F) +
geom_smooth(aes(y = Back_Gross_Profit, col = "back"), se = F) + ylab("") +
geom_smooth(aes(y = Total_Gross_Profit, col = "total"), se = F) +
geom_smooth(aes(y = .04*Cash_Price, col = "cash"), se = F)
Day<- as.data.frame(seq(as.Date("2016-01-01"), as.Date("2020-11-16"), by="days"))
colnames(Day)<-c("Date")
temp<- group_by(KDA1, Date) %>%
mutate(sum.fgp = sum(Front_Gross_Profit, na.rm = TRUE)) %>%
summarise(sum.fgp, .groups = 'drop') %>%
unique()
Day<-left_join(Day, count(group_by(KDA1, Date)), by = "Date")
Day<-left_join(Day, temp, by = "Date")
temp<- group_by(KDA1, Date) %>%
mutate(sum.bgp= sum(Back_Gross_Profit, na.rm = TRUE)) %>%
summarise(sum.bgp, .groups = 'drop') %>%
unique()
Day<-left_join(Day, temp, by = "Date")
temp<- group_by(KDA1, Date) %>%
mutate(sum.tgp = sum(Total_Gross_Profit, na.rm = TRUE)) %>%
summarise(sum.tgp, .groups = 'drop') %>%
unique()
Day<-left_join(Day, temp, by = "Date")
temp<- group_by(KDA1, Date) %>%
mutate(sum.cp = sum(Cash_Price, na.rm = TRUE)) %>%
summarise(sum.cp, .groups = 'drop') %>%
unique()
Day<-left_join(Day, temp, by = "Date")
Day<- as.data.frame(seq(as.Date("2016-01-01"), as.Date("2020-11-16"), by="days"))
colnames(Day)<-c("Date")
temp<- group_by(KDA1, Date) %>%
mutate(sum.fgp = sum(Front_Gross_Profit, na.rm = TRUE)) %>%
summarise(sum.fgp, .groups = 'drop') %>%
unique()
Day<-left_join(Day, count(group_by(KDA1, Date)), by = "Date")
Day<-left_join(Day, temp, by = "Date")
temp<- group_by(KDA1, Date) %>%
mutate(sum.bgp= sum(Back_Gross_Profit, na.rm = TRUE)) %>%
summarise(sum.bgp, .groups = 'drop') %>%
unique()
Day<-left_join(Day, temp, by = "Date")
temp<- group_by(KDA1, Date) %>%
mutate(sum.tgp = sum(Total_Gross_Profit, na.rm = TRUE)) %>%
summarise(sum.tgp, .groups = 'drop') %>%
unique()
Day<-left_join(Day, temp, by = "Date")
temp<- group_by(KDA1, Date) %>%
mutate(sum.cp = sum(Cash_Price, na.rm = TRUE)) %>%
summarise(sum.cp, .groups = 'drop') %>%
unique()
Day<-left_join(Day, temp, by = "Date")
summary(Day)
Day %>%
ggplot(aes(x = Date)) +
geom_smooth(aes(y =.6*sum.tgp-600, color = "total gross"),
se = F, method = 'loess') +
geom_smooth(aes(y = sum.bgp, color = "back gross"),
se = F, method = 'loess') +
geom_smooth(aes(y = .6*sum.fgp + 8700, color = "front gross"),
se = F, method = 'loess') +
geom_smooth(aes(y = .15*(sum.cp-183000), color = "cash price"),
se = F, method = 'loess') +
geom_smooth(aes(y = 1000*n+5000, color = "n sales"),
se = F, method = 'loess') + ylab("")
m<-'pmm'
junk<-capture.output(imp <- mice(data = Day, m = 20, method = c("",m,m,m,m,m)))
Day.full<-complete(imp, 20)
#The completed df
Day.full %>%
ggplot(aes(x = Date)) +
geom_smooth(aes(y =.6*sum.tgp-600, color = "total gross"),
se = F, method = 'loess') +
geom_smooth(aes(y = sum.bgp, color = "back gross"),
se = F, method = 'loess') +
geom_smooth(aes(y = .6*sum.fgp + 8700, color = "front gross"),
se = F, method = 'loess') +
geom_smooth(aes(y = .05*(sum.cp), color = "cash price"),
se = F, method = 'loess') +
geom_smooth(aes(y = 1000*n+5000, color = "n sales"),
se = F, method = 'loess') + ylab("")
Day.full %>%
ggplot(aes(x = Date)) +
geom_smooth(aes(y =.6*sum.tgp-600, color = "total gross"),
se = F, method = 'loess') +
geom_smooth(aes(y = sum.bgp, color = "back gross"),
se = F, method = 'loess') +
geom_smooth(aes(y = .6*sum.fgp + 8700, color = "front gross"),
se = F, method = 'loess') +
geom_smooth(aes(y = .15*(sum.cp-183000), color = "cash price"),
se = F, method = 'loess') +
geom_smooth(aes(y = 1000*n+5000, color = "n sales"),
se = F, method = 'loess') + ylab("")
#The completed df
Day.full %>%
ggplot(aes(x = Date)) +
geom_smooth(aes(y =.6*sum.tgp-600, color = "total gross"),
se = F, method = 'loess') +
geom_smooth(aes(y = sum.bgp, color = "back gross"),
se = F, method = 'loess') +
geom_smooth(aes(y = .6*sum.fgp + 8700, color = "front gross"),
se = F, method = 'loess') +
geom_smooth(aes(y = .15*(sum.cp-183000), color = "cash price"),
se = F, method = 'loess') +
geom_smooth(aes(y = 1000*n+5000, color = "n sales"),
se = F, method = 'loess') + ylab("")
Day %>%
ggplot(aes(x = Date)) +
geom_smooth(aes(y =.6*sum.tgp-600, color = "total gross"),
se = F, method = 'loess') +
geom_smooth(aes(y = sum.bgp, color = "back gross"),
se = F, method = 'loess') +
geom_smooth(aes(y = .6*sum.fgp + 8700, color = "front gross"),
se = F, method = 'loess') +
geom_smooth(aes(y = .15*(sum.cp-183000), color = "cash price"),
se = F, method = 'loess') +
geom_smooth(aes(y = 1000*n+5000, color = "n sales"),
se = F, method = 'loess') + ylab("")
b<-decompose(ts(Day.full$sum.bgp, frequency =365))
plot(b) + title("Back Gross", line = -1)
plot(b) + title("Back Gross", line = -.5)
f<-decompose(ts(Day.full$sum.fgp, frequency =365))
plot(f)+ title("Front Gross", line = -.5)
t<-decompose(ts(Day.full$sum.tgp, frequency =365))
plot(t) + title("Total Gross", line = -.5)
c<-decompose(ts(Day.full$sum.cp, frequency =365))
plot(c) + title("Cash Price", line = -.5)
n<-decompose(ts(Day.full$n, frequency =365))
plot(n) + title("N Sold", line =  -.5)
b<-decompose(ts(Day.full$sum.bgp, frequency =365))
plot(b) + title("Back Gross", line = -.5)
f<-decompose(ts(Day.full$sum.fgp, frequency =365))
plot(f)+ title("Front Gross", line = -.5)
t<-decompose(ts(Day.full$sum.tgp, frequency =365))
plot(t) + title("Total Gross", line = -.5)
c<-decompose(ts(Day.full$sum.cp, frequency =365))
plot(c) + title("Cash Price", line = -.5)
n<-decompose(ts(Day.full$n, frequency =365))
plot(n) + title("N Sold", line =  -.5)
#dummy for joining
time.seq<-seq.Date(from = as.Date("2016-01-01"), to = as.Date("2020-11-16"), by = 'day')
blank<-as.data.frame(time.seq)
colnames(blank)<-c("date")
## Then the final df
rogue<- cbind(as.data.frame(Day.full$Date), as.data.frame(b$trend),
as.data.frame(f$trend),
as.data.frame(t$trend), as.data.frame(c$trend),
as.data.frame(n$trend))
colnames(rogue)<-c("date", "backg", "frontg", "totalg", "cp", "n")
rogue<-left_join(blank, rogue)
#Finally, a very informative plot of our trends so far
plot(rogue)
ggplot(KDA1, aes(Date, Front_Gross_Profit)) + geom_point() + geom_smooth()
Day<- as.data.frame(seq(as.Date("2016-01-01"), as.Date("2020-11-16"), by="days"))
colnames(Day)<-c("Date")
temp<- group_by(KDA1, Date) %>%
mutate(sum.fgp = sum(Front_Gross_Profit, na.rm = TRUE)) %>%
summarise(sum.fgp, .groups = 'drop') %>%
unique()
Day<-left_join(Day, count(group_by(KDA1, Date)), by = "Date")
Day<-left_join(Day, temp, by = "Date")
temp<- group_by(KDA1, Date) %>%
mutate(sum.bgp= sum(Back_Gross_Profit, na.rm = TRUE)) %>%
summarise(sum.bgp, .groups = 'drop') %>%
unique()
Day<-left_join(Day, temp, by = "Date")
temp<- group_by(KDA1, Date) %>%
mutate(sum.tgp = sum(Total_Gross_Profit, na.rm = TRUE)) %>%
summarise(sum.tgp, .groups = 'drop') %>%
unique()
temp<- group_by(KDA1, Date) %>%
mutate(sum.cp = sum(Cash_Price, na.rm = TRUE)) %>%
summarise(sum.cp, .groups = 'drop') %>%
unique()
Day<-left_join(Day, temp, by = "Date")
Day<- as.data.frame(seq(as.Date("2016-01-01"), as.Date("2020-11-16"), by="days"))
colnames(Day)<-c("Date")
temp<- group_by(KDA1, Date) %>%
mutate(sum.fgp = sum(Front_Gross_Profit, na.rm = TRUE)) %>%
summarise(sum.fgp, .groups = 'drop') %>%
unique()
Day<-left_join(Day, count(group_by(KDA1, Date)), by = "Date")
Day<-left_join(Day, temp, by = "Date")
temp<- group_by(KDA1, Date) %>%
mutate(sum.bgp= sum(Back_Gross_Profit, na.rm = TRUE)) %>%
summarise(sum.bgp, .groups = 'drop') %>%
unique()
Day<-left_join(Day, temp, by = "Date")
temp<- group_by(KDA1, Date) %>%
mutate(sum.tgp = sum(Total_Gross_Profit, na.rm = TRUE)) %>%
summarise(sum.tgp, .groups = 'drop') %>%
unique()
Day<-left_join(Day, temp, by = "Date")
temp<- group_by(KDA1, Date) %>%
mutate(sum.cp = sum(Cash_Price, na.rm = TRUE)) %>%
summarise(sum.cp, .groups = 'drop') %>%
unique()
Day<-left_join(Day, temp, by = "Date")
### 4. Condensing
```{r}
Day<- as.data.frame(seq(as.Date("2016-01-01"), as.Date("2020-11-16"), by="days"))
colnames(Day)<-c("Date")
temp<- group_by(KDA1, Date) %>%
mutate(sum.fgp = sum(Front_Gross_Profit, na.rm = TRUE)) %>%
summarise(sum.fgp, .groups = 'drop') %>%
unique()
Day<-left_join(Day, count(group_by(KDA1, Date)), by = "Date")
Day<-left_join(Day, temp, by = "Date")
temp<- group_by(KDA1, Date) %>%
mutate(sum.bgp= sum(Back_Gross_Profit, na.rm = TRUE)) %>%
summarise(sum.bgp, .groups = 'drop') %>%
unique()
Day<-left_join(Day, temp, by = "Date")
temp<- group_by(KDA1, Date) %>%
mutate(sum.tgp = sum(Total_Gross_Profit, na.rm = TRUE)) %>%
summarise(sum.tgp, .groups = 'drop') %>%
unique()
Day<-left_join(Day, temp, by = "Date")
temp<- group_by(KDA1, Date) %>%
mutate(sum.cp = sum(Cash_Price, na.rm = TRUE)) %>%
summarise(sum.cp, .groups = 'drop') %>%
unique()
Day<-left_join(Day, temp, by = "Date")
```
#### There are 1782 days, with 55 NAs in the mix. Most of them are probably within
that stretch in April of 2019. We will fix them by imputation. First lets
take a look at this new df, now with the number of sales per day [n]. The missing
values will automatically be predicted using a simple linear model (loess).
```{r}
summary(Day)
Day %>%
ggplot(aes(x = Date)) +
geom_smooth(aes(y =.6*sum.tgp-600, color = "total gross"),
se = F, method = 'loess') +
geom_smooth(aes(y = sum.bgp, color = "back gross"),
se = F, method = 'loess') +
geom_smooth(aes(y = .6*sum.fgp + 8700, color = "front gross"),
se = F, method = 'loess') +
geom_smooth(aes(y = .15*(sum.cp-183000), color = "cash price"),
se = F, method = 'loess') +
geom_smooth(aes(y = 1000*n+5000, color = "n sales"),
se = F, method = 'loess') + ylab("")
```
### 5. Imputation
#### I've chosen to impute the mssing values so we can get a complete df that we
can run through functions and expect a complete output. To do this, we can use a
variety of methods. I've chosen to use predictive mean matching, which uses a
kind of regression that builds relationships around the mean of each variable.
Again, there are many ways to do this and this might not be the best one. First,
we train the algorithm 20 times [m = 20], and we get its best guess at what the
missing values would be according to pmm. We can easily change the method later too.
```{r, include=FALSE}
m<-'pmm'
junk<-capture.output(imp <- mice(data = Day, m = 20, method = c("",m,m,m,m,m)))
Day.full<-complete(imp, 20)
#The completed df
Day.full %>%
ggplot(aes(x = Date)) +
geom_smooth(aes(y =.6*sum.tgp-600, color = "total gross"),
se = F, method = 'loess') +
geom_smooth(aes(y = sum.bgp, color = "back gross"),
se = F, method = 'loess') +
geom_smooth(aes(y = .6*sum.fgp + 8700, color = "front gross"),
se = F, method = 'loess') +
geom_smooth(aes(y = .15*(sum.cp-183000), color = "cash price"),
se = F, method = 'loess') +
geom_smooth(aes(y = 1000*n+5000, color = "n sales"),
se = F, method = 'loess') + ylab("")
```
#### Now, since we have data for each day, we can do some timeries analysis for
all of our metrics. [freq = 365] because we have 365 observations per year.
Remember here, these values are taken from the sum of that value every day.
From, these plots, we really just need to pay attention to the trend.
```{r}
b<-decompose(ts(Day.full$sum.bgp, frequency =365))
plot(b) + title("Back Gross", line = -.5)
f<-decompose(ts(Day.full$sum.fgp, frequency =365))
plot(f)+ title("Front Gross", line = -.5)
t<-decompose(ts(Day.full$sum.tgp, frequency =365))
plot(t) + title("Total Gross", line = -.5)
c<-decompose(ts(Day.full$sum.cp, frequency =365))
plot(c) + title("Cash Price", line = -.5)
n<-decompose(ts(Day.full$n, frequency =365))
plot(n) + title("N Sold", line =  -.5)
```
#### Looks nice and informative. Now, lets make sure all our data is in the standard
style so we can easily compare it to other economic trends.
```{r}
#dummy for joining
time.seq<-seq.Date(from = as.Date("2016-01-01"), to = as.Date("2020-11-16"), by = 'day')
blank<-as.data.frame(time.seq)
colnames(blank)<-c("date")
## Then the final df
rogue<- cbind(as.data.frame(Day.full$Date), as.data.frame(b$trend),
as.data.frame(f$trend),
as.data.frame(t$trend), as.data.frame(c$trend),
as.data.frame(n$trend))
colnames(rogue)<-c("date", "backg", "frontg", "totalg", "cp", "n")
rogue<-left_join(blank, rogue)
#Finally, a very informative plot of our trends so far
plot(rogue)
```
#### Now, we are ready to test this df against some possible leading indicators.
As I understand it, leading indicators will show the current trend some number
of days, weeks, or even months. So instead  of just joining two df by date against
each other and grabbing the correlation from there, we'll want to modulate how far
in the past (or the future) we offset one df. Ideally, we would check the correlation
at a number of offset points, to  find the highest value we can get from the data.
I need to design a function to compile each new df and put it into a table of
correlations with LJ data.
#### I've chosen to impute the mssing values so we can get a complete df that we
can run through functions and expect a complete output. To do this, we can use a
variety of methods. I've chosen to use predictive mean matching, which uses a
kind of regression that builds relationships around the mean of each variable.
Again, there are many ways to do this and this might not be the best one. First,
we train the algorithm 20 times [m = 20], and we get its best guess at what the
missing values would be according to pmm. We can easily change the method later too.
```{r, include=FALSE}
m<-'pmm'
junk<-capture.output(imp <- mice(data = Day, m = 20, method = c("",m,m,m,m,m)))
Day.full<-complete(imp, 20)
#The completed df
Day.full %>%
ggplot(aes(x = Date)) +
geom_smooth(aes(y =.6*sum.tgp-600, color = "total gross"),
se = F, method = 'loess') +
geom_smooth(aes(y = sum.bgp, color = "back gross"),
se = F, method = 'loess') +
geom_smooth(aes(y = .6*sum.fgp + 8700, color = "front gross"),
se = F, method = 'loess') +
geom_smooth(aes(y = .15*(sum.cp-183000), color = "cash price"),
se = F, method = 'loess') +
geom_smooth(aes(y = 1000*n+5000, color = "n sales"),
se = F, method = 'loess') + ylab("")
```
#### Now, since we have data for each day, we can do some timeries analysis for
all of our metrics. [freq = 365] because we have 365 observations per year.
Remember here, these values are taken from the sum of that value every day.
From, these plots, we really just need to pay attention to the trend.
```{r}
b<-decompose(ts(Day.full$sum.bgp, frequency =365))
plot(b) + title("Back Gross", line = -.5)
f<-decompose(ts(Day.full$sum.fgp, frequency =365))
plot(f)+ title("Front Gross", line = -.5)
t<-decompose(ts(Day.full$sum.tgp, frequency =365))
plot(t) + title("Total Gross", line = -.5)
c<-decompose(ts(Day.full$sum.cp, frequency =365))
plot(c) + title("Cash Price", line = -.5)
n<-decompose(ts(Day.full$n, frequency =365))
plot(n) + title("N Sold", line =  -.5)
```
#### Looks nice and informative. Now, lets make sure all our data is in the standard
style so we can easily compare it to other economic trends.
```{r}
#dummy for joining
time.seq<-seq.Date(from = as.Date("2016-01-01"), to = as.Date("2020-11-16"), by = 'day')
blank<-as.data.frame(time.seq)
colnames(blank)<-c("date")
## Then the final df
rogue<- cbind(as.data.frame(Day.full$Date), as.data.frame(b$trend),
as.data.frame(f$trend),
as.data.frame(t$trend), as.data.frame(c$trend),
as.data.frame(n$trend))
colnames(rogue)<-c("date", "backg", "frontg", "totalg", "cp", "n")
rogue<-left_join(blank, rogue)
#Finally, a very informative plot of our trends so far
plot(rogue)
```
#### Now, we are ready to test this df against some possible leading indicators.
As I understand it, leading indicators will show the current trend some number
of days, weeks, or even months. So instead  of just joining two df by date against
each other and grabbing the correlation from there, we'll want to modulate how far
in the past (or the future) we offset one df. Ideally, we would check the correlation
at a number of offset points, to  find the highest value we can get from the data.
I need to design a function to compile each new df and put it into a table of
correlations with LJ data.
#dummy for joining
time.seq<-seq.Date(from = as.Date("2016-01-01"), to = as.Date("2020-11-16"), by = 'day')
blank<-as.data.frame(time.seq)
colnames(blank)<-c("date")
## Then the final df
rogue<- cbind(as.data.frame(Day.full$Date), as.data.frame(b$trend),
as.data.frame(f$trend),
as.data.frame(t$trend), as.data.frame(c$trend),
as.data.frame(n$trend))
colnames(rogue)<-c("date", "backg", "frontg", "totalg", "cp", "n")
rogue<-left_join(blank, rogue)
#Finally, a very informative plot of our trends so far
plot(rogue)
