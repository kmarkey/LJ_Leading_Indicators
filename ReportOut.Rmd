---
title: 'Leading Indicators for LJ Kirkland'
subtitle: 'First Report'
author: "Keaton Markey"
date:  "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document:
    latex_engine: xelatex
    toc: true
    number_sections: false
    toc_depth: 6
    highlight: tango
  html_document:
    toc: yes
urlcolor: blue
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup}
library(here)
library(ggplot2)
library(stringr)
library(lubridate)
library(dplyr)
library(logger)
library(here)
library(scales)
library(tidyr)
library(readr)
```
# Introduction:

### Project Aim:

Find external indicators that provide reliable insight into previous business
performance of LJ, and use those indicators to predict future business performance.


### Project Overview:



# Methods:

## 1. Digest a Prepare Data

Raw sales data provided by LJ has been cleaned and summarized by month.

## 2. Find Indicators

Potential candidates were chosen from a pool of stocks, economic
measures, Google Trends data, and some internal
appointment information. The process of selecting which candidates to examine was based on,
other similar projects, current market research, and suggestions from experts.

While finding leading indicators is one of the primary goals of the project, it is also a necessary step to achieve the second goal. One of the most important concerns when prediction and training Machine Learning models is called _overfitting_. This is a phenomenon that occurs when the model performs well on the training data, but performs much more poorly when it gets new information.

The data that was analyzed for this project is especially prone to inducing overfitting due to its width. Because there are more features and relatively few observations, extra measures were taken to reduce overfitting.

After compiling a list of suggested candidates, candidates that met the following criteria were excluded:
-   Updated less frequently than every month
-   Not released before the 5th of the month
-   Not available for the duration of LJ data (2010-present)

To identify candidates as leading indicators and permit use in each model, a variety of _feature selection_ techniques were employed:
-   correlation coefficient
-   Lasso L1 regularization
-   Permutation Feature Importance

## 3. Create Predictions

After vetting each candidate, they were assembled into a dataset and given to each of the following models:
a)  Linear Model (with L1 regularization)
b)  Decision Tree
c)  Random Forest
d)  LSTM
e)  Ensemble Model: Binary Classification & GRU (BCGRU)

Each model utilizes different algorithms to establish a relationship between the input and output. Thus, they must be interpreted differently. Some guidelines for model interpretation and additional considerations are described below.

### Linear Model with L1 regularization


### Decision Tree

### Random Forest

### LSTM

### BCGRU

# Results

# Figure Appendix

# Indicator Index

```{r, label="Set Bounds", include=FALSE, echo = FALSE}

KDAc <- read_csv("data/sour/KDAc.csv")
here()

# generated on
today <- Sys.Date()

# last month
lmonth <- floor_date(floor_date(today, unit = "month") - 1, unit = "month")

# bounds of 2nd to last month on record
nmonth <- floor_date(floor_date(max(KDAc$date), unit = "month") - 1, unit = "month")
nomonth <- ceiling_date(nmonth, unit = "month") - 1

# bounds of last month on record
pmonth <- nomonth + 1
pqmonth <- ceiling_date(pmonth, unit = "month") - 1

# report would be made on the 1st of
repon <- pqmonth + 1

# report is for the month of
repfor <- paste0(month(pqmonth, label = TRUE, abbr = FALSE), ", ", year(pqmonth))
```

```{r theme set, include = FALSE, echo = FALSE}
pal <- c("#f6aa1c","#08415c","#6b818c","#eee5e9","#ba7ba1","#c28cae","#a52a2a")

blue <- "#114482"
lightblue <- "#146ff8"
llightblue <- "#AFCFFF"
red <- "#a52a2a"
white <- "#FBFFF1"
yellow <- "#F6AA1C"
green <- "#588157"

ljtheme <- function(){
    theme_minimal() %+replace%
    theme(
      panel.grid.major = element_line(linetype = "solid", color = llightblue, linewidth = 0.1),
      panel.grid.minor = element_blank(),
      panel.background = element_rect(fill = white), #light
      panel.border = element_rect(color = lightblue, fill = NA, linetype = "solid", linewidth = 2),
      legend.background = element_rect(fill = white, color = lightblue, linewidth = 1), # legend
      legend.text = element_text(color = blue),
      legend.title = element_text(face = "bold.italic", color = blue),
      legend.position = "bottom",
      legend.key = element_rect(fill = white),
      
      
      strip.text.x = element_text(size = 12, color = white, face = "bold.italic"),
      strip.text.y = element_text(size = 12, color = white, face = "bold.italic"),
      text = element_text(color = white),
      axis.title = element_text(face = "italic", size = 11, color = white), 
      axis.text = element_text(color = white, size = 9), 
      axis.ticks = element_line(color = white, linewidth = .5, lineend = "butt"), 
      axis.ticks.length = unit(.1, "cm"),
      plot.title = element_text(face = "bold", # labels
                              color = white, size = 14, hjust = 0, vjust = 1.5),
      plot.subtitle = element_text(color = white, hjust = 0, vjust = 1.5, face = "italic"),
      plot.caption = element_text(color = white, face = "bold"),
      plot.background = element_rect(fill = blue)
    )
}

# A function factory for getting integer y-axis values.
integer_breaks <- function(n = 5, ...) {
  fxn <- function(x) {
    breaks <- floor(pretty(x, n, ...))
    names(breaks) <- attr(breaks, "labels")
    breaks
  }
  return(fxn)
}

theme_set(ljtheme())
```

## 1. Finding the Features

After processing sales data, all external indicators are fetched from the
following sources and API packages monthly:

-   Quandl
-   AlphaVantage
    -   with the alpha_vantage library
-   Google Trends
    -   with modifications to the pytrends library
-   St. Louis Federal Reserve 

A full list of candidates, as well as which predictions they support, can be found in the Feature Appendix.


Each indicator is lagged at 3, 6, 9,and 12 month intervals to  create multiple candidates. Each candidate is
evaluated for correlation to the target
metric. Any candidate that does not meet the correlation threshold is
eliminated as the first pass of feature selection.

Features are further selected for each model independently in the
following ways:

-   Linear Regression

    -   Each remaining feature is passed through a Lasso regularization
        process, which selects features based on the efficacy of linear
        models that contain them using 5-fold cross-validation.

-   Decision Tree

    -   Regularization is applied after training when the tree is pruned
        at a depth optimized by 5-fold cross-validation.

-   

    ## Random Forest


```{r}

new_used <- "NEW"
purchase_lease <- "L"
metric <- "n"
group <- "weekly" # day, quarter and year?

group_unit <- function(group) {
  if (group == "daily") {
    return("day")
  } else return(substr(group, 1, nchar(group) - 2))
}

data <- KDAc %>%

    {if (new_used != "BOTH")
        
        dplyr::filter(., nu == new_used) # this and last month
      else .
        } %>%
    
    {if (purchase_lease != "BOTH")
        
        dplyr::filter(., pl == purchase_lease)
      else .
      } %>%
  
  group_by(!!group := floor_date(date, group_unit(!!group))) %>%
        
  {if (metric == "n")
    dplyr::summarise(., n = n(), date)
    else
      dplyr::summarise(., !!metric := mean(get(metric), na.rm = TRUE), date)}
  
data %>%
  ggplot() +
      geom_line(aes(x = get(group), y = get(metric)), color = blue, linewidth = 2) +
      geom_point(aes(x = get(group), y = get(metric)), color = blue, size = 1.5) +
      geom_line(aes(x = get(group), y = get(metric)), color = lightblue, linewidth = .5) +



  
  labs(x = "",
       y = paste0(metric))

```

```{r new.used movers}
#### rolling average
#https://stackoverflow.com/questions/33769770/use-rollapply-and-zoo-to-calculate-rolling-average-of-a-column-of-variables

# Change in sales for the 5 most common vehicles new/used

# don't really care about latency, we can dynamically create plots based on parameters

new_used <- "NEW"
movers_n <- 10

# biggest movers from last month ??

movers <- KDAc %>%
  
  {if (new_used != "BOTH")
    dplyr::filter(., nu == new_used, date >= nmonth, date <= pqmonth) # this and last month
    else 
      dplyr::filter(., date >= nmonth, date <= pqmonth)
  } %>%
  
  {if (new_used == "USED") 
    dplyr::mutate(., carname = str_c(make, " ", model), .keep = 'unused')
    else # Only by model, not by year
      dplyr::mutate(., carname = str_c(make, " ", model, " ", caryear), .keep = 'unused')} %>%
  
  dplyr::group_by(carname, month = ifelse(month(date) == month(pqmonth), "current", "last")) %>% # change month names
  
  dplyr::summarise(n = n()) %>%
  
  pivot_wider(id_cols = "carname", names_from = "month", values_from = "n", values_fill = 0) %>%
  
  dplyr::mutate(change = current - last) %>%
  ungroup() %>%
  slice_max(n = movers_n, order_by = abs(change), with_ties = FALSE)

movers %>%
  
  ggplot() + 
  
  geom_segment(aes(x = reorder(carname, change), y = last, xend = carname, yend = current, color = change > 0),
               linewidth = 10, lineend = "round") +
  
  geom_point(aes(x = carname, y = current), color = "black", alpha = 0.2, size  = 10) +

  
  geom_segment(aes(x = reorder(carname, change), y = ifelse(change > 0, current + 2, current - 2), xend = carname, yend = last),
               color = "white",
               linewidth = 1.1, lineend = "round", linejoin = "bevel",
               arrow = arrow(length = unit(0.15, "in"), ends = "first")) +
  
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5),
        legend.position = "none") +
  
  scale_color_manual(values = c(red, green)) +
  
  labs(title = paste0("Biggest Movers This Month (", new_used, ")"),
       subtitle = repfor,
       x = "",
       y = "Change In Sales")

```

```{r Last 6 Months Sales}

# n.months must be less than as.numeric(month(pmonth, abbr = FALSE, label = FALSE))
n.months <- 6

metric <- "n"

metric_dict <- c(n = "Total Sales", 
                 front_gross_profit = "Avg. Front Gross Profit", 
                 back_gross_profit = "Avg. Back Gross Profit",
                 total_gross_profit = "Avg. Total Gross Profit",
                 cash_price = "Avg. Cash Price")

mlabs <- function(x) {
  paste0(month(
    pmonth - months(as.numeric(x)),
    label = TRUE,
    abbr = FALSE
  ))
}

mgroups <- KDAc %>%
  
  dplyr::mutate(mgroup = factor(month(pmonth) - month(date), ordered = TRUE),
                theyear = year(date)) %>%
  
  group_by(mgroup, theyear) %>%
  
  {if (metric == "n")
    summarise(., y = n(), mgroup, theyear)
    else
      summarise(., y = mean(get(metric), na.rm = TRUE), mgroup, theyear)} %>%

  distinct() %>%
  
  dplyr::filter(theyear >= year(pmonth) - 1, mgroup <= n.months - 1 &&
                mgroup >= 0)

ggplot() +
  geom_bar(
    data = mgroups %>% dplyr::filter(theyear == year(pmonth)),
    aes(x = mgroup, y = y),
    width = 0.7,
    stat = "identity",
    fill = lightblue,
    alpha = 0.8
  ) +
  
  geom_line(
    data = mgroups %>% dplyr::filter(theyear == year(pmonth) - 1),
    aes(x = mgroup, y = y, group = theyear),
    lwd = 1.5,
    lty = 9,
    alpha = 0.6
  ) +
  
  geom_label(
    data = mgroups %>% dplyr::filter(theyear == year(pmonth) - 1, mgroup == n.months - 1),
    aes(x = mgroup, y = y - 10, label = "Last Year"),
    fill = "black",
    color = "white"
  ) +
  
  {if (metric == "n") 
      geom_label(data = mgroups %>% dplyr::filter(theyear == year(pmonth)),
             aes(x = mgroup, y = y, label = comma(round(y))))
    else
      geom_label(data = mgroups %>% dplyr::filter(theyear == year(pmonth)),
                 aes(x = mgroup, y = y, label = paste0("$", comma(round(y)))))
    } +
  
  scale_x_discrete(labels = mlabs, limits = rev) +
  
  scale_y_continuous(labels = comma) +
  
  labs(
    title = paste0(metric_dict[metric], " in the Last ", n.months, " Months"),
    subtitle = paste0("From ", repfor),
    y = '',
    x = ''
  )
```

Compare this month to other years

```{r, label="This Month's Rank Among Other Years"}
calmonth <- KDAc %>%
  dplyr::filter(month(date) == month(pmonth), year(date) <= year(pmonth)) %>%
  group_by(theyear = year(date)) %>%
  dplyr::transmute(`Avg. Cash Price` = mean(cash_price, na.rm = TRUE),
                   `Total Gross` = sum(total_gross_profit, na.rm = TRUE),
                   `Back Gross` = sum(back_gross_profit, na.rm = TRUE),
                   `Front Gross` = sum(front_gross_profit, na.rm = TRUE),
                   `New Sales` = sum(ifelse(nu == "NEW", 1, 0)),
                   `Used Sales` = sum(ifelse(nu == "USED", 1, 0)),
                   ) %>%
  distinct() %>%
  
  pivot_longer(!theyear, names_to = "name", values_to = "value") %>%
  group_by(name = factor(name, levels = c("Total Gross", # ordered
                                          "Back Gross",
                                          "Front Gross",
                                          "Avg. Cash Price",
                                          "New Sales",
                                          "Used Sales"), ordered = TRUE)) %>%
  
  mutate(ranks = order(order(value, decreasing = TRUE)), # actual ranks
         rankval = rank(value)) # numeric bar chart value

ggplot(calmonth) +
  geom_bar(aes(x = theyear, y = rankval, fill = rankval),  stat = "identity") + 
  facet_wrap(~name) +
  geom_text(aes(x = theyear, y = rankval/2, label = ranks), color = white) +
  scale_fill_gradient2(midpoint = 3, low = red, mid = llightblue,
                     high = blue) +
  labs(title = paste0("Ranking ", repfor),
       subtitle = paste0("Compared to other '", month(pmonth, label = TRUE, abbr = FALSE), "'s"),
       x = '',
       y = '') +
  theme(legend.position = "none",
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank())
```

```{r, label="People"}
# best salesman by avg cash price? avg fgp? n sales? for this month

salesman_metric <- "n" # button!

salesman.n <- 5

salesman_metric_dict <-
  c(n = "Total Sales", fgp_a = "Avg. Front Gross Profit", cp_a = "Avg. Cash Price")

salespeople <- KDAc %>%
  dplyr::filter(date >= pmonth, date <= pqmonth) %>%
  group_by(salesman) %>%
  dplyr::transmute(
    cp_a = mean(cash_price, na.rm = TRUE),
    fgp_a = mean(front_gross_profit, na.rm = TRUE),
    n = n(),
    salesman
  ) %>%
  distinct() %>%
  ungroup() %>%
  slice_max(n = salesman.n, order_by = get(salesman_metric))

salespeople %>%
  ggplot() + geom_bar(aes(x = reorder(salesman, get(salesman_metric)),
                          y = get(salesman_metric)),
                      stat = "identity", fill = lightblue) +
  {
    if (salesman_metric == "n")
      geom_label(aes(
        x = salesman,
        y = get(salesman_metric),
        label = paste0(salesman, "\n", round(get(salesman_metric), digits = 0))
      ),
      size = 3)
  } +
  {
    if (salesman_metric != "n")
      geom_label(aes(
        x = salesman,
        y = get(salesman_metric),
        label = paste0(salesman, "\n$", scales::comma(round(
          get(salesman_metric), digits = 0
        )))
      ),
      size = 3)
  } +
  # geom_label(aes(x = salesman,
  #                y = get(salesman_metric),
  #                label = paste0(salesman, "\n", scales::comma(round(get(salesman_metric), digits = 0)))),
  #            size = 3) +
  
  labs(
    title = paste("Top Salesman By", salesman_metric_dict[salesman_metric]),
    subtitle = repfor,
    y = salesman_metric_dict[salesman_metric],
    x = ""
  ) +
  scale_y_continuous(expand = expansion(mult = c(-0.5, 0.05)),
                     breaks = integer_breaks()) +
  theme(axis.text.x = element_blank(),
        axis.ticks.x = element_blank())
  
# best fi manager by back gross?

fi.n <- 5

fipeople <- KDAc %>%
  dplyr::filter(date >= pmonth, date <= pqmonth) %>%
  group_by(fimanager) %>%
  dplyr::transmute(bgp_a = mean(back_gross_profit, na.rm = TRUE), fimanager) %>%
  distinct() %>%
  ungroup() %>%
  slice_max(n = fi.n, order_by = bgp_a)

fipeople %>%
  ggplot() + geom_bar(aes(x = reorder(fimanager, bgp_a),
                          y = bgp_a),
                      stat = "identity", fill = lightblue) +
  geom_label(aes(
    x = fimanager,
    y = bgp_a,
    label = paste0(fimanager, "\n $", scales::comma(round(bgp_a, digits = 0)))
  ),
  size = 3) +
  
  labs(
    title = paste("Top Finance Manager By Avg. Back Gross Profit"),
    subtitle = repfor,
    y = "Back Gross Profit",
    x = ""
  ) +
  scale_y_continuous(expand = expansion(mult = c(-0.5, 0.05)),
                     labels = comma) +
  theme(axis.text.x = element_blank(),
        axis.ticks.x = element_blank())
```

# Next Month?

Based on past business performance cycles, months + years

```{r, label="Next Month", eval = FALSE}
# years on right, months on left?
#
nmonth_metric <- "Back Gross"

estimates_for <- paste0(month(nmonth, abbr = FALSE, label = TRUE), ", ", year(nmonth))

# last 2 months, last 2 years
predate <- as.character(c(as.character(rev(month((nmonth) - months(1:2), label = TRUE, abbr = FALSE))), # last 2 months
                          estimates_for,
                          (year(nmonth) - 1):year((nmonth) - years(2)))) # last 2 years

# group 2 insert for next month
thismonthinsert <- list(group = 2, 
                        tframe = estimates_for, 
                        "Avg. Cash Price" = NULL,
                        "Total Gross" = NULL, 
                        "Back Gross" = NULL,
                        "Front Gross" = NULL, 
                        "New Sales" = NULL,
                        "Used Sales" = NULL)

# previous months + years for next month

nmonth_df <- bind_rows(KDAc %>% # last 2 months
                   dplyr::filter(date >= pmonth - months(1), date < pqmonth) %>%
                   dplyr::mutate(group = 1, tframe = as.character(month(date, label = TRUE, abbr = FALSE))),
                 KDAc %>% # last 2 years
                   dplyr::filter(month(date) == month(pmonth) + 1, year(date) >= year(pmonth) - 2, date < pqmonth) %>%
                   dplyr::mutate(group = 3, tframe = as.character(year(date)))) %>%
  group_by(group, tframe) %>%
  summarise(`Avg. Cash Price` = mean(cash_price, na.rm = TRUE),
            `Total Gross` = sum(total_gross_profit, na.rm = TRUE),
            `Back Gross` = sum(back_gross_profit, na.rm = TRUE),
            `Front Gross` = sum(front_gross_profit, na.rm = TRUE),
            `New Sales` = sum(ifelse(nu == "NEW", 1, 0)),
            `Used Sales` = sum(ifelse(nu == "USED", 1, 0))) %>%
  bind_rows(thismonthinsert) %>%
  dplyr::mutate(tframe = factor(tframe, levels = predate, order = TRUE)) %>%
  pivot_longer(cols = -any_of(c("group", "tframe")), names_to = "class", values_to = "value") %>%
  group_by(group, class) %>%
  dplyr::mutate(a = ifelse(group != 2, coef(lm(value ~ as.numeric(tframe)))[2], 0),
                b = ifelse(group != 2, coef(lm(value ~ as.numeric(tframe)))[1], 0))

if (!(nmonth_metric %in% nmonth_df$class)) {
  warning("Enter a valid class")
} 

# prediction function
predrectf <- function(facet) {
  g1 <- predict(lm(value ~ as.numeric(tframe), data = nmonth_df, subset = (group == 1 & class == nmonth_metric)), list(tframe = 3))
  g3 <- predict(lm(value ~ as.numeric(tframe), data = nmonth_df, subset = (group == 3 & class == nmonth_metric)), list(tframe = 3))
  return(c(group1 = g1, group3 = g3))
}

# get bounding box
predrect <- predrectf()

nmonth_df %>%
  filter(class == nmonth_metric) %>%
  ggplot() + 
  geom_point(aes(x = tframe, y = value, fill = factor(group), shape = factor(group)),
             size = 5,
             color = "black",
             inherit.aes = FALSE) +
  scale_shape_manual(values = c(25, 2, 22)) +
  geom_abline(aes(slope = a, intercept = b, color = factor(group)),
             lty = 2, size = 2) +
  theme(axis.text.x = element_text(angle = 15),
        legend.position = "none") +
  geom_rect(aes(xmin = 2.8, xmax = 3.2, ymin = min(predrect), ymax = max(predrect)),  # should always be between 5.5 and 6.5
            stat = "identity", 
            data = nmonth_df %>%
             filter(group == 2), 
            inherit.aes = FALSE, 
            alpha = 0.1,
            color = "black") +
  scale_y_continuous(labels = comma) + 
  coord_cartesian(ylim = c(min(predrect) * 0.7, max(predrect) * 1.3)) +
  labs(title = paste0(nmonth_metric, " Estimate for This Month"),
       subtitle = paste0(month(repon, abbr = FALSE, label = TRUE), ", ", year(repon)),
       x = "",
       y = "Value")
```

# Predictions

```{r}
# read in imaginary df(s)


```


```{r, label="Cumulative Tracker"}

```

### 2. Summary statistics

```{r Summary}
```

#### We have some NA's, or empty spaces, in the data. This could be for a

#### We can use logistic regression to tell us how much the NAs might influence

the data longitudinally. This is probably unnecessary but I'm going to
do it anyway.



#### Based on the models, we noticed a significant trend for all the metrics.

It shouldn't impact analysis too much because we didn't test numeric
variables, but its something to keep in mind.

Generally, we saw more missing Front Gross Profit values and Cash Price
values at the end of the data set, and we saw more missing values for
Back Gross Profit and Total Gross Profit at the beginning of the data
set.

### 3. Plotting

#### Now, lets look at some quick plots of the data.

```{r}
# ggplot(KDA1, aes(Date, Front_Gross_Profit)) + geom_point() + geom_smooth()
# ggplot(KDA1, aes(Date, Back_Gross_Profit)) + geom_point() + geom_smooth()
# ggplot(KDA1, aes(Date, Total_Gross_Profit)) + geom_point() + geom_smooth()
# ggplot(KDA1, aes(Date, Cash_Price)) + geom_point() + geom_smooth()
```

#### 

```{r}
# ggplot(KDA1, aes(x = Date)) + geom_smooth(aes(y = Front_Gross_Profit, col = "front"), se = F) +
#   geom_smooth(aes(y = Back_Gross_Profit, col = "back"), se = F) + ylab("") +
#   geom_smooth(aes(y = Total_Gross_Profit, col = "total"), se = F) +
#   geom_smooth(aes(y = .04*Cash_Price, col = "cash"), se = F) 
```

#### If we hope to do anything with this, we will have to extract these trends.

To make this easier, lets create a new df for each day.

### 4. Condensing

```{r}
# Day<- as.data.frame(seq(as.Date("2016-01-01"), as.Date("2020-11-16"), by="days"))
# colnames(Day)<-c("Date")
# temp<- group_by(KDA1, Date) %>%
#   mutate(sum.fgp = sum(Front_Gross_Profit, na.rm = TRUE)) %>%
#   summarise(sum.fgp, .groups = 'drop') %>%
#   unique()
# Day<-left_join(Day, count(group_by(KDA1, Date)), by = "Date")
# Day<-left_join(Day, temp, by = "Date")
# temp<- group_by(KDA1, Date) %>%
#   mutate(sum.bgp= sum(Back_Gross_Profit, na.rm = TRUE)) %>%
#   summarise(sum.bgp, .groups = 'drop') %>%
#   unique()
# Day<-left_join(Day, temp, by = "Date")
# temp<- group_by(KDA1, Date) %>%
#   mutate(sum.tgp = sum(Total_Gross_Profit, na.rm = TRUE)) %>%
#   summarise(sum.tgp, .groups = 'drop') %>%
#   unique()
# Day<-left_join(Day, temp, by = "Date")
# temp<- group_by(KDA1, Date) %>%
#   mutate(sum.cp = sum(Cash_Price, na.rm = TRUE)) %>%
#   summarise(sum.cp, .groups = 'drop') %>%
#   unique()
# Day<-left_join(Day, temp, by = "Date")
```

#### There are 1782 days, with 55 NAs in the mix. Most of them are probably within

that stretch in April of 2019. We will fix them by imputation. First
lets take a look at this new df, now with the number of sales per day
[n]. The missing values will automatically be predicted using a simple
linear model (loess).

```{r}
# summary(Day)
# Day %>%
# ggplot(aes(x = Date)) + 
#   geom_smooth(aes(y =.6*sum.tgp-600, color = "total gross"), 
#               se = F, method = 'loess') +
#   geom_smooth(aes(y = sum.bgp, color = "back gross"), 
#               se = F, method = 'loess') + 
#   geom_smooth(aes(y = .6*sum.fgp + 8700, color = "front gross"), 
#               se = F, method = 'loess') +
#   geom_smooth(aes(y = .15*(sum.cp-183000), color = "cash price"), 
#               se = F, method = 'loess') +
#   geom_smooth(aes(y = 1000*n+5000, color = "n sales"), 
#               se = F, method = 'loess') + ylab("")
```

### 5. Imputation

#### I've chosen to impute the mssing values so we can get a complete df that we

can run through functions and expect a complete output. To do this, we
can use a variety of methods. I've chosen to use predictive mean
matching, which uses a kind of regression that builds relationships
around the mean of each variable. Again, there are many ways to do this
and this might not be the best one. First, we train the algorithm 20
times [m = 20], and we get its best guess at what the missing values
would be according to pmm. We can easily change the method later too.

```{r, include=FALSE}
# m<-'pmm'
# junk<-capture.output(imp <- mice(data = Day, m = 20, method = c("",m,m,m,m,m)))
# Day.full<-complete(imp, 20)
# #The completed df
# Day.full %>%
# ggplot(aes(x = Date)) + 
#   geom_smooth(aes(y =.6*sum.tgp-600, color = "total gross"), 
#               se = F, method = 'loess') +
#   geom_smooth(aes(y = sum.bgp, color = "back gross"), 
#               se = F, method = 'loess') + 
#   geom_smooth(aes(y = .6*sum.fgp + 8700, color = "front gross"), 
#               se = F, method = 'loess') +
#   geom_smooth(aes(y = .15*(sum.cp-183000), color = "cash price"), 
#               se = F, method = 'loess') +
#   geom_smooth(aes(y = 1000*n+5000, color = "n sales"), 
#               se = F, method = 'loess') + ylab("")
```

#### Now, since we have data for each day, we can do some timeries analysis for

all of our metrics. [freq = 365] because we have 365 observations per
year. Remember here, these values are taken from the sum of that value
every day. From, these plots, we really just need to pay attention to
the trend.

```{r}
# b<-decompose(ts(Day.full$sum.bgp, frequency =365))
# plot(b) + title("Back Gross", line = -.5)
# f<-decompose(ts(Day.full$sum.fgp, frequency =365))
# plot(f)+ title("Front Gross", line = -.5)
# t<-decompose(ts(Day.full$sum.tgp, frequency =365))
# plot(t) + title("Total Gross", line = -.5)
# c<-decompose(ts(Day.full$sum.cp, frequency =365))
# plot(c) + title("Cash Price", line = -.5)
# n<-decompose(ts(Day.full$n, frequency =365))
# plot(n) + title("N Sold", line =  -.5)
```

#### Looks nice and informative. Now, lets make sure all our data is in the standard

style so we can easily compare it to other economic trends.

```{r}
#dummy for joining
# time.seq<-seq.Date(from = as.Date("2016-01-01"), to = as.Date("2020-11-16"), by = 'day')
# blank<-as.data.frame(time.seq)
# colnames(blank)<-c("date")
# ## Then the final df
# rogue<- cbind(as.data.frame(Day.full$Date), as.data.frame(b$trend),
#               as.data.frame(f$trend),
#               as.data.frame(t$trend), as.data.frame(c$trend), 
#               as.data.frame(n$trend))
# colnames(rogue)<-c("date", "backg", "frontg", "totalg", "cp", "n")
# rogue<-left_join(blank, rogue)
# #Finally, a very informative plot of our trends so far
# plot(rogue)
```

# Data Sources

```{r, include=FALSE}
# read in json  files
library(rjson)

stock_info <- fromJSON(file = "data/out/stocks_info.json")

fred_info <- fromJSON(file = "data/out/fred_info.json")

trend_info <- fromJSON(file = "data/out/trends_info.json")

get_info <- function(data) {

df <- data.frame()

    for (i in names(data)){
    
        for (j in data[i]) {
            r <- c(key = i, unlist(j))
            }
        df <- rbind(df, r)
    }
    
    names(df) <- c("key", names(data[[1]]))
    
    return(df)
}

info_df <- rbind(get_info(stock_info),
                 get_info(fred_info),
                 get_info(trend_info)) %>%
    dplyr::summarise(name, key, category, updated, `citation`, link)

 # add in best lag and usage per model here

```

```{r display-info, results='asis'}
for (i in 1:nrow(info_df)) {
  cat("\n\n##[", info_df$name[i], "](", info_df$link[i], ")", sep = "")
  cat("\n_", info_df$key[i], "_", sep = "")
  cat("\n__", info_df$category[i], "_", sep = "")
  cat("\n", info_df$updated[i], sep = "")
  cat("\n", info_df$citation[i], sep = "")
}
```

# References

# Credit

Author: Keaton Markey Version: 0.11 Date Revised: 1/11/2023 Property of:
Lee Johnson  , Kirkland WA
