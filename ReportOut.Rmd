---
title: 'Leading Indicators for LJ Kirkland'
subtitle: 'Preliminary Analysis'
author: "Keaton Markey"
date:  "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document:
    latex_engine: xelatex
    toc: true
    number_sections: true
    toc_depth: 6
    highlight: tango
  html_document:
    toc: yes
urlcolor: blue
---

#### Statement

From the last 5 years of data, we are trying to find external indicators of a 
variety of business metrics within the Lee Johnson Auto Family's Kirkland location.
I will gather a number of longitudinal economic data sets and match them against a 
few metrics provided by the company.

#### 

As is common with data science, there is a lot of ways to do things. First I will 
clean and organize the data to extract general trends that will be used to check 
correlations. The second part of my approach will center around finding an external 
leading indicator using simple correlations 
that identify datasets that have high similarity to trends within the company.
The third part of this analysis with use some basic techniques in machine learning.
To make sure the chosen indicator(s) are accurate and useful, we will train a model  
so we can effectively predict future trends.

####

I will focus not on informational analysis of LJ data, but how these trends reflect and
respond to the economy and various other factors. Compiling a list of possible 
with readily available data will be the most challenging part of this project.

#### Lets begin with librarying in packages that we will need for the first part.

## Part 1.

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  cache = FALSE, # if TRUE knitr will cache results to reuse in future knits
  fig.width = 6, # the width for plots created by code chunk
  fig.height = 4.5, # the height for plots created by code chunk
  fig.align = 'center', # how to align graphics. 'left', 'right', 'center'
  dpi = 150, 
  results = 'asis', # knitr passes through results without reformatting
  echo = TRUE, # if FALSE knitr won't display code in chunk above it's results
  message = TRUE, # if FALSE knitr won't display messages generated by code
  strip.white = TRUE, # if FALSE knitr won't remove white spaces at beg or end of code chunk
  warning = FALSE) # if FALSE knitr won't display warning messages in the doc
```

### 1. Packages

#### These will allow us to plot, impute missing data.
#### tidyverse(), mice(), and Quandl()

```{r, include = FALSE}
library(tidyverse)
library(mice)
```

#### Read in data

```{r}
KDARaw <- read.csv("C:/Users/keato/Dropbox/Shop Data/Keaton Data Analysis Project-2016-2020.csv", 
                   na.strings = c("", "-", "--", "---", "	 -   ", " -   ", " ", "  ", strip.white = TRUE))
colnames(KDARaw) <- (c("Date", "DealNum", "VStock", "Year", "Make",
                       "Model", "NU", "Front_Gross_Profit", "Back_Gross_Profit",
                       "Total_Gross_Profit", "Cash_Price", "PL", "Sale_Type",
                       "Salesman", "Salesmanager", "FIManager"))
#remove first row
KDARaw<-as.data.frame(KDARaw)
KDARaw<- KDARaw[-c(1),]
#remove parenthesis and change numeric parsing of columns
pear<- function(x){
  x<- gsub("[()]", "", x)
  x<- gsub(" ", "", x)
  x<- as.numeric(gsub(",", "", x))
}
KDARaw$Front_Gross_Profit<-pear(KDARaw$Front_Gross_Profit)
KDARaw$Back_Gross_Profit<-pear(KDARaw$Back_Gross_Profit)
KDARaw$Total_Gross_Profit<- pear(KDARaw$Total_Gross_Profit)
KDARaw$Cash_Price<-pear(KDARaw$Cash_Price)
KDARaw$Date<-as.Date(KDARaw$Date, format = "%m/%d/%Y")
KDARaw$PL<-as.factor(KDARaw$PL)
KDARaw<- arrange(KDARaw, Date)
#create a test df
KDA1<-KDARaw
```

### 2. Summary statistics

```{r}
summary(KDA1)
glimpse(KDA1)
```

#### We have some NA's, or empty spaces, in the data. This could be for a 
variety of reasons, someone didn't enter in a number, it got lost in translation, 
or just didn't exist in the first place. Before we deal with these missing values, 
let's see how much they affect our data. We really only care about NAs in numerical 
columns, because thats when they could affect the stats we will run later.

#### We can use logistic regression to tell us how much the NAs might influence
the data longitudinally. This is probably unnecessary but I'm going to do it anyway.
```{r}
summary(glm(is.na(Back_Gross_Profit) ~ Date, KDA1, family = 'binomial'))
summary(glm(is.na(Front_Gross_Profit) ~ Date, KDA1, family = 'binomial'))
summary(glm(is.na(Total_Gross_Profit) ~ Date, KDA1, family = 'binomial'))
summary(glm(is.na(Cash_Price) ~ Date, KDA1, family = 'binomial'))

```
#### Based on the models, we noticed a significant trend for all the metrics.
It shouldn't impact analysis too much because we didn't test numeric variables, 
but its something to keep in mind.

  Generally, we saw more missing Front Gross Profit values and Cash Price values
  at the end of the data set, and we saw more missing values for Back Gross Profit
  and Total Gross Profit at the beginning of the data set.
  
### 3. Plotting

#### Now, lets look at some quick plots of the data.
```{r}
ggplot(KDA1, aes(Date, Front_Gross_Profit)) + geom_point() + geom_smooth()
ggplot(KDA1, aes(Date, Back_Gross_Profit)) + geom_point() + geom_smooth()
ggplot(KDA1, aes(Date, Total_Gross_Profit)) + geom_point() + geom_smooth()
ggplot(KDA1, aes(Date, Cash_Price)) + geom_point() + geom_smooth()
```
#### Things are a little hard to see with all 20,000 of these 
data points. The smoothing spline helps a bit, but we can do even better.
```{r}
ggplot(KDA1, aes(x = Date)) + geom_smooth(aes(y = Front_Gross_Profit, col = "front"), se = F) +
  geom_smooth(aes(y = Back_Gross_Profit, col = "back"), se = F) + ylab("") +
  geom_smooth(aes(y = Total_Gross_Profit, col = "total"), se = F) +
  geom_smooth(aes(y = .04*Cash_Price, col = "cash"), se = F) 
```
#### If we hope to do anything with this, we will have to extract these trends. 
To make this easier, lets create a new df for each day.

### 4. Condensing
```{r}
Day<- as.data.frame(seq(as.Date("2016-01-01"), as.Date("2020-11-16"), by="days"))
colnames(Day)<-c("Date")
temp<- group_by(KDA1, Date) %>%
  mutate(sum.fgp = sum(Front_Gross_Profit, na.rm = TRUE)) %>%
  summarise(sum.fgp, .groups = 'drop') %>%
  unique()
Day<-left_join(Day, count(group_by(KDA1, Date)), by = "Date")
Day<-left_join(Day, temp, by = "Date")
temp<- group_by(KDA1, Date) %>%
  mutate(sum.bgp= sum(Back_Gross_Profit, na.rm = TRUE)) %>%
  summarise(sum.bgp, .groups = 'drop') %>%
  unique()
Day<-left_join(Day, temp, by = "Date")
temp<- group_by(KDA1, Date) %>%
  mutate(sum.tgp = sum(Total_Gross_Profit, na.rm = TRUE)) %>%
  summarise(sum.tgp, .groups = 'drop') %>%
  unique()
Day<-left_join(Day, temp, by = "Date")
temp<- group_by(KDA1, Date) %>%
  mutate(sum.cp = sum(Cash_Price, na.rm = TRUE)) %>%
  summarise(sum.cp, .groups = 'drop') %>%
  unique()
Day<-left_join(Day, temp, by = "Date")
```
#### There are 1782 days, with 55 NAs in the mix. Most of them are probably within
that stretch in April of 2019. We will fix them by imputation. First lets
take a look at this new df, now with the number of sales per day [n]. The missing
values will automatically be predicted using a simple linear model (loess).
```{r}
summary(Day)
Day %>%
ggplot(aes(x = Date)) + 
  geom_smooth(aes(y =.6*sum.tgp-600, color = "total gross"), 
              se = F, method = 'loess') +
  geom_smooth(aes(y = sum.bgp, color = "back gross"), 
              se = F, method = 'loess') + 
  geom_smooth(aes(y = .6*sum.fgp + 8700, color = "front gross"), 
              se = F, method = 'loess') +
  geom_smooth(aes(y = .15*(sum.cp-183000), color = "cash price"), 
              se = F, method = 'loess') +
  geom_smooth(aes(y = 1000*n+5000, color = "n sales"), 
              se = F, method = 'loess') + ylab("")
```
### 5. Imputation

#### I've chosen to impute the mssing values so we can get a complete df that we
can run through functions and expect a complete output. To do this, we can use a 
variety of methods. I've chosen to use predictive mean matching, which uses a
kind of regression that builds relationships around the mean of each variable.
Again, there are many ways to do this and this might not be the best one. First,
we train the algorithm 20 times [m = 20], and we get its best guess at what the 
missing values would be according to pmm. We can easily change the method later too.
```{r, include=FALSE}
m<-'pmm'
junk<-capture.output(imp <- mice(data = Day, m = 20, method = c("",m,m,m,m,m)))
Day.full<-complete(imp, 20)
#The completed df
Day.full %>%
ggplot(aes(x = Date)) + 
  geom_smooth(aes(y =.6*sum.tgp-600, color = "total gross"), 
              se = F, method = 'loess') +
  geom_smooth(aes(y = sum.bgp, color = "back gross"), 
              se = F, method = 'loess') + 
  geom_smooth(aes(y = .6*sum.fgp + 8700, color = "front gross"), 
              se = F, method = 'loess') +
  geom_smooth(aes(y = .15*(sum.cp-183000), color = "cash price"), 
              se = F, method = 'loess') +
  geom_smooth(aes(y = 1000*n+5000, color = "n sales"), 
              se = F, method = 'loess') + ylab("")
```
#### Now, since we have data for each day, we can do some timeries analysis for
all of our metrics. [freq = 365] because we have 365 observations per year.
Remember here, these values are taken from the sum of that value every day.
From, these plots, we really just need to pay attention to the trend.
```{r}
b<-decompose(ts(Day.full$sum.bgp, frequency =365))
plot(b) + title("Back Gross", line = -.5)
f<-decompose(ts(Day.full$sum.fgp, frequency =365))
plot(f)+ title("Front Gross", line = -.5)
t<-decompose(ts(Day.full$sum.tgp, frequency =365))
plot(t) + title("Total Gross", line = -.5)
c<-decompose(ts(Day.full$sum.cp, frequency =365))
plot(c) + title("Cash Price", line = -.5)
n<-decompose(ts(Day.full$n, frequency =365))
plot(n) + title("N Sold", line =  -.5)
```
#### Looks nice and informative. Now, lets make sure all our data is in the standard
style so we can easily compare it to other economic trends.
```{r}
#dummy for joining
time.seq<-seq.Date(from = as.Date("2016-01-01"), to = as.Date("2020-11-16"), by = 'day')
blank<-as.data.frame(time.seq)
colnames(blank)<-c("date")
## Then the final df
rogue<- cbind(as.data.frame(Day.full$Date), as.data.frame(b$trend),
              as.data.frame(f$trend),
              as.data.frame(t$trend), as.data.frame(c$trend), 
              as.data.frame(n$trend))
colnames(rogue)<-c("date", "backg", "frontg", "totalg", "cp", "n")
rogue<-left_join(blank, rogue)
#Finally, a very informative plot of our trends so far
plot(rogue)
```
#### Now, we are ready to test this df against some possible leading indicators.
  As I understand it, leading indicators will show the current trend some number 
  of days, weeks, or even months. So instead  of just joining two df by date against
  each other and grabbing the correlation from there, we'll want to modulate how far
  in the past (or the future) we offset one df. Ideally, we would check the correlation
  at a number of offset points, to  find the highest value we can get from the data.
  I need to design a function to compile each new df and put it into a table of 
  correlations with LJ data.

## Part 2.
```{r}







```









