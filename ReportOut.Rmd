---
title: 'Leading Indicators'
subtitle: 'First Report'
author: "Keaton Markey"
date:  "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document:
    latex_engine: xelatex
    toc: true
    number_sections: false
    toc_depth: 6
    highlight: tango
  html_document:
    toc: yes
urlcolor: blue
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup}
library(here)
library(ggplot2)
library(stringr)
library(lubridate)
library(dplyr)
library(logger)
library(here)
library(scales)
library(tidyr)
library(readr)
```

# Introduction:

### Project Aim:

Find external indicators that provide reliable insight into previous
business performance of LJ, and use those indicators to predict future
business performance.

### Project Overview:

# Methods:

## 1. Digest & Prepare Data

Raw sales data, cleaned and summarized by month. Potential indicators
were grouped by data source and summarized by month. They are refreshed
each month.

## 2. Find Indicators

Potential candidates were identified by other similar projects, current
market research, and suggestions from experts. They were selected from a
pool of stocks, economic measures, Google Trends, and some internal
appointment information. Finding leading indicators is also a necessary
step to create accurate and reliable predictions.

After compiling a list of potential candidates, candidates that met the
following criteria were excluded:

-   Updated less frequently than every month
-   Not released before the 5th of the month
-   Not available for the duration of available data (2010-present)

Identifying leading indicators can be translated to a machine learning
problem called *feature selection*. The feature selection processes used in this project sometimes combine conventional statistics and machine learning methods.

Candidates were all passed through a low correlation filter of \>
0.20, and only the two indicators with the highest correlation from each
source were selected. After a candidate has passed these filters, it is conventionally called a _feature_.

Then, as a final measure of identifying leading indicators, the following machine learning techniques were employed:

-   Lasso L1 Regularization
-   Impurity pruning
-   Permutation Feature Importance

## 3. Create Predictions

After identifying candidates, each of the following models was paired
with a feature selection technique and trained on the assembled data:

a)  Linear Model
b)  Decision Tree
c)  Random Forest
d)  LSTM
e)  GRU

Each model utilizes different methods to establish a relationship
between the candidates and business performance. Thus, they must be
interpreted differently. Some guidelines for model interpretation and
additional model considerations are described below.

### Linear Model with L1 regularization

The linear model establishes a linear relationship between each feature
and the output. This is the most simple method and typically very
effective. Feature selection was performed with 5-fold cross-validation
through Lasso regression, optimized for mean-squared-error.

The leading indicators used in this model are best described as those with the strongest linear relationship to business performance.

### Decision Tree

The decision tree is another simpler method of prediction. The current
model is illustrated in a function below.

###### PHOTO 

5-fold cross-validation was used to select tree depth and the optimal number of features. The model is scored with mean-squared-error. Additionally, this model is more resistant to monthly changes and can be interpreted as a slow-moving estimation. The leading indicators used in this model can be interpreted as those with the greatest impact on minimizing the residual error of the model.

### Random Forest

A random forest is a collection of hundreds of randomly generated decision trees that sums up votes from each tree. This method is less sensitive to large changes due to the voting system. Each tree uses between 3 and 4 features randomly selected from a pool of all features.

### LSTM

A LSTM (Long-Short-Term-Memory) is a deep neural network that is well-suited to the problem of time-series prediction. This is the most complex model used in this project. Due to the small size of the data, the model is heavily throttled by hidden layer size and training duration.

These can be described as leading indicators with the greatest impact on on minimizing the residual error of the model.

### GRU

A GRU is the younger brother to the LSTM, with a little less complexity. The model uses the same hyperparameters as the LSTM.

# Results



# Discussion

One of the most important concerns when prediction and training machine
learning models is called *overfitting*. This is a phenomenon that
occurs when the model fits the training data very well, but performs
poorly when asked to predict new data. In other words, the algorithms
are able to establish relationships that exist in the data from the
past, but can't be generalized to make accurate predictions about the
future. Every kind of model is prone to overfitting, and the models in this project were especially at risk of creating this problem. One way to prevent overfitting is to usemore data. Adding more data to the model is the best way to increase
generalization. The current models are trained on between 100 and 120
monthly sales totals, which leaves much to be desired. 

I would be comfortable to deploy the prediction phase of this project when the test scores are, on average no more than 100% larger than the training scores.

_________ prediction power ______________

Another way to reduce overfitting is by removing some features. This was done by selecting only the most important features (leading indicators) from over 300 candidates. With less features to choose from, models estimate important relationships instead of just calculating changes in the data.

With so many candidates for leading indicators, more monthly data could dramatically affect the output of feature
selection and model performance tests. Often methods like bootstrapping and sampling are used to increase the number of observations, but with time series data there is little that can be done without disrupting the sequential nature of observations. We could also considered other intervals, such as weekly or bi-weekly, but would likely introduce more variance and restrict candidate selection even further.

Additionally, with all the data the models use, it is possible that there is more economic information, stock trades, or other metric that may prove useful. More features may still be necessary to improve prediction of business performance and reduce overfitting.
In the future, including more monthly observations and more feature candidates is necessary for improving this project.

## COVID-19

In April of 2020, the United States entered a lockdown. This is a problem for two reasons. First, only two sales in the month of April 2020, which is a big outlier. Normally, sales hover between 200 and 400 per month. Such a big deviation affects machine learning algorithms greatly, and instead of extracting relationships across the whole time period, they would focus on the one or two months that are outliers. I have avoided this issue by removing a couple of months while training, but its important to note that these months aren't included in the model.

The second and more serious issue is how the pandemic disrupted the economy. While it had a direct impact on sales for this business, it also disrupted the business relationship between groups across the world. Many organizations underwent a period of rapid change, where few emerged operating exactly how they did before  April 2020.

The challenge with trying to predict business performance during this time is that whatever unidentified relationships existed between businesses before the pandemic have been changed or dissolved entirely. Currently, the models use the time before the pandemic to predict the business performance all future months. Without a way to quantify relationships that emerged after April 2020, the models make the assumption that the landscape of candidates and their relationship to this business has not changed since then.


# 

# Figure Appendix

# Indicator Index

```{r, label="Set Bounds", include=FALSE, echo = FALSE}

KDAc <- read_csv("data/sour/KDAc.csv")
here()

# generated on
today <- Sys.Date()

# last month
lmonth <- floor_date(floor_date(today, unit = "month") - 1, unit = "month")

# bounds of 2nd to last month on record
nmonth <- floor_date(floor_date(max(KDAc$date), unit = "month") - 1, unit = "month")
nomonth <- ceiling_date(nmonth, unit = "month") - 1

# bounds of last month on record
pmonth <- nomonth + 1
pqmonth <- ceiling_date(pmonth, unit = "month") - 1

# report would be made on the 1st of
repon <- pqmonth + 1

# report is for the month of
repfor <- paste0(month(pqmonth, label = TRUE, abbr = FALSE), ", ", year(pqmonth))
```

```{r theme set, include = FALSE, echo = FALSE}
pal <- c("#f6aa1c","#08415c","#6b818c","#eee5e9","#ba7ba1","#c28cae","#a52a2a")

blue <- "#114482"
lightblue <- "#146ff8"
llightblue <- "#AFCFFF"
red <- "#a52a2a"
white <- "#FBFFF1"
yellow <- "#F6AA1C"
green <- "#588157"

ljtheme <- function(){
    theme_minimal() %+replace%
    theme(
      panel.grid.major = element_line(linetype = "solid", color = llightblue, linewidth = 0.1),
      panel.grid.minor = element_blank(),
      panel.background = element_rect(fill = white), #light
      panel.border = element_rect(color = lightblue, fill = NA, linetype = "solid", linewidth = 2),
      legend.background = element_rect(fill = white, color = lightblue, linewidth = 1), # legend
      legend.text = element_text(color = blue),
      legend.title = element_text(face = "bold.italic", color = blue),
      legend.position = "bottom",
      legend.key = element_rect(fill = white),
      
      
      strip.text.x = element_text(size = 12, color = white, face = "bold.italic"),
      strip.text.y = element_text(size = 12, color = white, face = "bold.italic"),
      text = element_text(color = white),
      axis.title = element_text(face = "italic", size = 11, color = white), 
      axis.text = element_text(color = white, size = 9), 
      axis.ticks = element_line(color = white, linewidth = .5, lineend = "butt"), 
      axis.ticks.length = unit(.1, "cm"),
      plot.title = element_text(face = "bold", # labels
                              color = white, size = 14, hjust = 0, vjust = 1.5),
      plot.subtitle = element_text(color = white, hjust = 0, vjust = 1.5, face = "italic"),
      plot.caption = element_text(color = white, face = "bold"),
      plot.background = element_rect(fill = blue)
    )
}

# A function factory for getting integer y-axis values.
integer_breaks <- function(n = 5, ...) {
  fxn <- function(x) {
    breaks <- floor(pretty(x, n, ...))
    names(breaks) <- attr(breaks, "labels")
    breaks
  }
  return(fxn)
}

theme_set(ljtheme())
```

## 1. Finding the Features

After processing sales data, all external indicators are fetched from
the following sources and API packages monthly:

-   Quandl
-   AlphaVantage
    -   with the alpha_vantage library
-   Google Trends
    -   with modifications to the pytrends library
-   St. Louis Federal Reserve

A full list of candidates, as well as which predictions they support,
can be found in the Feature Appendix.

Each indicator is lagged at 3, 6, 9,and 12 month intervals to create
multiple candidates. Each candidate is evaluated for correlation to the
target metric. Any candidate that does not meet the correlation
threshold is eliminated as the first pass of feature selection.

Features are further selected for each model independently in the
following ways:

-   Linear Regression

    -   Each remaining feature is passed through a Lasso regularization
        process, which selects features based on the efficacy of linear
        models that contain them using 5-fold cross-validation.

-   Decision Tree

    -   Regularization is applied after training when the tree is pruned
        at a depth optimized by 5-fold cross-validation.

-   

    ## Random Forest

```{r}

# 6 months
# 1 yr
# 5 yr
# all time

new_used <- "NEW"
purchase_lease <- "L"
metric <- "n"
group <- "weekly" # day, quarter and year?

group_unit <- function(group) {
  if (group == "daily") {
    return("day")
  } else return(substr(group, 1, nchar(group) - 2))
}

data <- KDAc %>%

    {if (new_used != "BOTH")
        
        dplyr::filter(., nu == new_used) # this and last month
      else .
        } %>%
    
    {if (purchase_lease != "BOTH")
        
        dplyr::filter(., pl == purchase_lease)
      else .
      } %>%
  
  group_by(!!group := floor_date(date, group_unit(!!group))) %>%
        
  {if (metric == "n")
    dplyr::summarise(., n = n(), date)
    else
      dplyr::summarise(., !!metric := mean(get(metric), na.rm = TRUE), date)}
  
data %>%
  ggplot() +
      geom_line(aes(x = get(group), y = get(metric)), color = blue, linewidth = 2) +
      geom_point(aes(x = get(group), y = get(metric)), color = blue, size = 1.5) +
      geom_line(aes(x = get(group), y = get(metric)), color = lightblue, linewidth = .5) +



  
  labs(x = "",
       y = paste0(metric))

```

```{r new.used movers}
#### rolling average
#https://stackoverflow.com/questions/33769770/use-rollapply-and-zoo-to-calculate-rolling-average-of-a-column-of-variables

# Change in sales for the 5 most common vehicles new/used

# don't really care about latency, we can dynamically create plots based on parameters

new_used <- "NEW"
movers_n <- 10

# biggest movers from last month ??

movers <- KDAc %>%
  
  {if (new_used != "BOTH")
    dplyr::filter(., nu == new_used, date >= nmonth, date <= pqmonth) # this and last month
    else 
      dplyr::filter(., date >= nmonth, date <= pqmonth)
  } %>%
  
  {if (new_used == "USED") 
    dplyr::mutate(., carname = str_c(make, " ", model), .keep = 'unused')
    else # Only by model, not by year
      dplyr::mutate(., carname = str_c(make, " ", model, " ", caryear), .keep = 'unused')} %>%
  
  dplyr::group_by(carname, month = ifelse(month(date) == month(pqmonth), "current", "last")) %>% # change month names
  
  dplyr::summarise(n = n()) %>%
  
  pivot_wider(id_cols = "carname", names_from = "month", values_from = "n", values_fill = 0) %>%
  
  dplyr::mutate(change = current - last) %>%
  ungroup() %>%
  slice_max(n = movers_n, order_by = abs(change), with_ties = FALSE)

movers %>%
  
  ggplot() + 
  
  geom_segment(aes(x = reorder(carname, change), y = last, xend = carname, yend = current, color = change > 0),
               linewidth = 10, lineend = "round") +
  
  geom_point(aes(x = carname, y = current), color = "black", alpha = 0.2, size  = 10) +

  
  geom_segment(aes(x = reorder(carname, change), y = ifelse(change > 0, current + 2, current - 2), xend = carname, yend = last),
               color = "white",
               linewidth = 1.1, lineend = "round", linejoin = "bevel",
               arrow = arrow(length = unit(0.15, "in"), ends = "first")) +
  
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5),
        legend.position = "none") +
  
  scale_color_manual(values = c(red, green)) +
  
  labs(title = paste0("Biggest Movers This Month (", new_used, ")"),
       subtitle = repfor,
       x = "",
       y = "Change In Sales")

```

```{r Last 6 Months Sales}
# n.months must be less than as.numeric(month(pmonth, abbr = FALSE, label = FALSE))
n.months <- 3

years <- 7

metric <- "n"

metric_dict <- c(
  n = "Total Sales",
  front_gross_profit = "Avg. Front Gross Profit",
  back_gross_profit = "Avg. Back Gross Profit",
  total_gross_profit = "Avg. Total Gross Profit",
  cash_price = "Avg. Cash Price"
)

mlabs <- function(x) {
  paste0(month(
    pmonth - months(as.numeric(x)),
    label = TRUE,
    abbr = FALSE
  ))
}

month(pmonth)

# group by distance from current month
mgroups <- KDAc %>%
  
  dplyr::mutate(mgroup = factor(month(pmonth) - month(date), ordered = TRUE),
                theyear = year(date)) %>%
  
  group_by(mgroup, theyear) %>%
  
  {
    if (metric == "n")
      summarise(., y = n(), mgroup, theyear)
    else
      summarise(., y = mean(get(metric), na.rm = TRUE), mgroup, theyear)
  } %>%
  
  distinct() %>%
  
  dplyr::filter(theyear >= year(pmonth) - years + 1,
                as.numeric(as.character(mgroup)) >= 0 && as.numeric(as.character(mgroup)) <= n.months - 1)

mgroups

ggplot() +
  geom_col(
    data = mgroups,
    aes(
      x = rev(mgroup),
      y = y,
      group = theyear,
      fill = theyear
    ),
    width = 0.8,
    position = 'dodge',
  ) +
  
  geom_text(
    data = mgroups,
    aes(
      x = rev(mgroup),
      y = 0,
      group = theyear,
      label = theyear, 
      color = y < 5 
    ),
    position = position_dodge(width = 0.8),
    angle = 90,
    hjust = -0.1
  ) +

  scale_x_discrete(labels = mlabs, limits = rev) +
  
  scale_y_continuous(labels = comma) +
  
  scale_color_manual(values = c("white", "black")) +
  
  labs(y = '',
       x = '') +
  theme(legend.position = 'none')

```

Compare this month to other years

```{r, label="This Month's Rank Among Other Years"}
calmonth <- KDAc %>%
  dplyr::filter(month(date) == month(pmonth), year(date) <= year(pmonth)) %>%
  group_by(theyear = year(date)) %>%
  dplyr::transmute(`Avg. Cash Price` = mean(cash_price, na.rm = TRUE),
                   `Total Gross` = sum(total_gross_profit, na.rm = TRUE),
                   `Back Gross` = sum(back_gross_profit, na.rm = TRUE),
                   `Front Gross` = sum(front_gross_profit, na.rm = TRUE),
                   `New Sales` = sum(ifelse(nu == "NEW", 1, 0)),
                   `Used Sales` = sum(ifelse(nu == "USED", 1, 0)),
                   ) %>%
  distinct() %>%
  
  pivot_longer(!theyear, names_to = "name", values_to = "value") %>%
  group_by(name = factor(name, levels = c("Total Gross", # ordered
                                          "Back Gross",
                                          "Front Gross",
                                          "Avg. Cash Price",
                                          "New Sales",
                                          "Used Sales"), ordered = TRUE)) %>%
  
  mutate(ranks = order(order(value, decreasing = TRUE)), # actual ranks
         rankval = rank(value)) # numeric bar chart value

ggplot(calmonth) +
  geom_bar(aes(x = theyear, y = rankval, fill = rankval),  stat = "identity") + 
  facet_wrap(~name) +
  geom_text(aes(x = theyear, y = rankval/2, label = ranks), color = white) +
  scale_fill_gradient2(midpoint = 3, low = red, mid = llightblue,
                     high = blue) +
  labs(title = paste0("Ranking ", repfor),
       subtitle = paste0("Compared to other '", month(pmonth, label = TRUE, abbr = FALSE), "'s"),
       x = '',
       y = '') +
  theme(legend.position = "none",
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank())
```

```{r, label="People"}
# best salesman by avg cash price? avg fgp? n sales? for this month

salesman_metric <- "number_of_sales" # button!

salesman.n <- 5

salesman_metric_dict <-
  c(n = "Total Sales", fgp_a = "Avg. Front Gross Profit", cp_a = "Avg. Cash Price")

salespeople <- KDAc %>%
  dplyr::filter(date >= pmonth, date <= pqmonth) %>%
  group_by(salesman) %>%
  dplyr::transmute(
    cp_a = mean(cash_price, na.rm = TRUE),
    fgp_a = mean(front_gross_profit, na.rm = TRUE),
    n = n(),
    salesman
  ) %>%
  distinct() %>%
  ungroup() %>%
  slice_max(n = salesman.n, order_by = get(salesman_metric))

salespeople %>%
  ggplot() + geom_bar(aes(x = reorder(salesman, get(salesman_metric)),
                          y = get(salesman_metric)),
                      stat = "identity", fill = lightblue) +
  {
    if (salesman_metric == "n")
      geom_label(aes(
        x = salesman,
        y = get(salesman_metric),
        label = paste0(salesman, "\n", round(get(salesman_metric), digits = 0))
      ),
      size = 3)
  } +
  {
    if (salesman_metric != "n")
      geom_label(aes(
        x = salesman,
        y = get(salesman_metric),
        label = paste0(salesman, "\n$", scales::comma(round(
          get(salesman_metric), digits = 0
        )))
      ),
      size = 3)
  } +
  # geom_label(aes(x = salesman,
  #                y = get(salesman_metric),
  #                label = paste0(salesman, "\n", scales::comma(round(get(salesman_metric), digits = 0)))),
  #            size = 3) +
  
  labs(
    title = paste("Top Salesman By", salesman_metric_dict[salesman_metric]),
    subtitle = repfor,
    y = salesman_metric_dict[salesman_metric],
    x = ""
  ) +
  scale_y_continuous(expand = expansion(mult = c(-0.5, 0.05)),
                     breaks = integer_breaks()) +
  theme(axis.text.x = element_blank(),
        axis.ticks.x = element_blank())
  
# best fi manager by back gross?

fi.n <- 5

fipeople <- KDAc %>%
  dplyr::filter(date >= pmonth, date <= pqmonth) %>%
  group_by(fimanager) %>%
  dplyr::transmute(bgp_a = mean(back_gross_profit, na.rm = TRUE), fimanager) %>%
  distinct() %>%
  ungroup() %>%
  slice_max(n = fi.n, order_by = bgp_a)

fipeople %>%
  ggplot() + geom_bar(aes(x = reorder(fimanager, bgp_a),
                          y = bgp_a),
                      stat = "identity", fill = lightblue) +
  geom_label(aes(
    x = fimanager,
    y = bgp_a,
    label = paste0(fimanager, "\n $", scales::comma(round(bgp_a, digits = 0)))
  ),
  size = 3) +
  
  labs(
    title = paste("Top Finance Manager By Avg. Back Gross Profit"),
    subtitle = repfor,
    y = "Back Gross Profit",
    x = ""
  ) +
  scale_y_continuous(expand = expansion(mult = c(-0.5, 0.05)),
                     labels = comma) +
  theme(axis.text.x = element_blank(),
        axis.ticks.x = element_blank())
```

```{r}
data <- KDAc %>%

  dplyr::filter(date >= pmonth, date <= pqmonth) %>%
  
  group_by(salesman) %>%
  
  # metric
  {
    if (salesman_metric == "number_of_sales")
      
      dplyr::summarise(., metric = n())
    
    else
      
      dplyr::summarise(., metric = mean(get(salesman_metric), na.rm = TRUE))
  } %>%
  
  distinct() %>%
  
  slice_max(n = salesman.n,
            order_by = metric)


data
# plotting
data %>%
  ggplot() + geom_bar(aes(x = reorder(salesman, metric),
                          y = metric),
                      stat = "identity") +
  
  {
    if (salesman_metric == "number_of_sales")
      geom_label(aes(
        x = salesman,
        y = metric,
        label = paste0(salesman, "\n", round(metric, digits = 0))
      ),
      size = 3)
    
    else
      geom_label(
        aes(
        x = salesman,
        y = metric,
        label = paste0(salesman, "\n$", scales::comma(round(metric, digits = 0)))
      ),
      size = 3)
  } +
  # geom_label(aes(x = salesman,
  #                y = get(salesman_metric),
  #                label = paste0(salesman, "\n", scales::comma(round(get(salesman_metric), digits = 0)))),
  #            size = 3) +
  
  labs(y = "",
       x = "") +
  
  scale_y_continuous(expand = expansion(mult = c(-0.3, 0.1)),
                     breaks = integer_breaks()) +
  
  theme(axis.text.x = element_blank(),
        axis.ticks.x = element_blank())
```

# Next Month?

Based on past business performance cycles, months + years

```{r, label="Next Month", eval = FALSE}
# years on right, months on left?
#
nmonth_metric <- "Back Gross"

estimates_for <- paste0(month(nmonth, abbr = FALSE, label = TRUE), ", ", year(nmonth))

# last 2 months, last 2 years
predate <- as.character(c(as.character(rev(month((nmonth) - months(1:2), label = TRUE, abbr = FALSE))), # last 2 months
                          estimates_for,
                          (year(nmonth) - 1):year((nmonth) - years(2)))) # last 2 years

# group 2 insert for next month
thismonthinsert <- list(group = 2, 
                        tframe = estimates_for, 
                        "Avg. Cash Price" = NULL,
                        "Total Gross" = NULL, 
                        "Back Gross" = NULL,
                        "Front Gross" = NULL, 
                        "New Sales" = NULL,
                        "Used Sales" = NULL)

# previous months + years for next month

nmonth_df <- bind_rows(KDAc %>% # last 2 months
                   dplyr::filter(date >= pmonth - months(1), date < pqmonth) %>%
                   dplyr::mutate(group = 1, tframe = as.character(month(date, label = TRUE, abbr = FALSE))),
                 KDAc %>% # last 2 years
                   dplyr::filter(month(date) == month(pmonth) + 1, year(date) >= year(pmonth) - 2, date < pqmonth) %>%
                   dplyr::mutate(group = 3, tframe = as.character(year(date)))) %>%
  group_by(group, tframe) %>%
  summarise(`Avg. Cash Price` = mean(cash_price, na.rm = TRUE),
            `Total Gross` = sum(total_gross_profit, na.rm = TRUE),
            `Back Gross` = sum(back_gross_profit, na.rm = TRUE),
            `Front Gross` = sum(front_gross_profit, na.rm = TRUE),
            `New Sales` = sum(ifelse(nu == "NEW", 1, 0)),
            `Used Sales` = sum(ifelse(nu == "USED", 1, 0))) %>%
  bind_rows(thismonthinsert) %>%
  dplyr::mutate(tframe = factor(tframe, levels = predate, order = TRUE)) %>%
  pivot_longer(cols = -any_of(c("group", "tframe")), names_to = "class", values_to = "value") %>%
  group_by(group, class) %>%
  dplyr::mutate(a = ifelse(group != 2, coef(lm(value ~ as.numeric(tframe)))[2], 0),
                b = ifelse(group != 2, coef(lm(value ~ as.numeric(tframe)))[1], 0))

if (!(nmonth_metric %in% nmonth_df$class)) {
  warning("Enter a valid class")
} 

# prediction function
predrectf <- function(facet) {
  g1 <- predict(lm(value ~ as.numeric(tframe), data = nmonth_df, subset = (group == 1 & class == nmonth_metric)), list(tframe = 3))
  g3 <- predict(lm(value ~ as.numeric(tframe), data = nmonth_df, subset = (group == 3 & class == nmonth_metric)), list(tframe = 3))
  return(c(group1 = g1, group3 = g3))
}

# get bounding box
predrect <- predrectf()

nmonth_df %>%
  filter(class == nmonth_metric) %>%
  ggplot() + 
  geom_point(aes(x = tframe, y = value, fill = factor(group), shape = factor(group)),
             size = 5,
             color = "black",
             inherit.aes = FALSE) +
  scale_shape_manual(values = c(25, 2, 22)) +
  geom_abline(aes(slope = a, intercept = b, color = factor(group)),
             lty = 2, size = 2) +
  theme(axis.text.x = element_text(angle = 15),
        legend.position = "none") +
  geom_rect(aes(xmin = 2.8, xmax = 3.2, ymin = min(predrect), ymax = max(predrect)),  # should always be between 5.5 and 6.5
            stat = "identity", 
            data = nmonth_df %>%
             filter(group == 2), 
            inherit.aes = FALSE, 
            alpha = 0.1,
            color = "black") +
  scale_y_continuous(labels = comma) + 
  coord_cartesian(ylim = c(min(predrect) * 0.7, max(predrect) * 1.3)) +
  labs(title = paste0(nmonth_metric, " Estimate for This Month"),
       subtitle = paste0(month(repon, abbr = FALSE, label = TRUE), ", ", year(repon)),
       x = "",
       y = "Value")
```

# Predictions

```{r}
# read in imaginary df(s)


```

```{r, label="Cumulative Tracker"}

```

### 2. Summary statistics

```{r Summary}
```

#### We have some NA's, or empty spaces, in the data. This could be for a

#### We can use logistic regression to tell us how much the NAs might influence

the data longitudinally. This is probably unnecessary but I'm going to
do it anyway.

#### Based on the models, we noticed a significant trend for all the metrics.

It shouldn't impact analysis too much because we didn't test numeric
variables, but its something to keep in mind.

Generally, we saw more missing Front Gross Profit values and Cash Price
values at the end of the data set, and we saw more missing values for
Back Gross Profit and Total Gross Profit at the beginning of the data
set.

### 3. Plotting

#### Now, lets look at some quick plots of the data.

```{r}
# ggplot(KDA1, aes(Date, Front_Gross_Profit)) + geom_point() + geom_smooth()
# ggplot(KDA1, aes(Date, Back_Gross_Profit)) + geom_point() + geom_smooth()
# ggplot(KDA1, aes(Date, Total_Gross_Profit)) + geom_point() + geom_smooth()
# ggplot(KDA1, aes(Date, Cash_Price)) + geom_point() + geom_smooth()
```

#### 

```{r}
# ggplot(KDA1, aes(x = Date)) + geom_smooth(aes(y = Front_Gross_Profit, col = "front"), se = F) +
#   geom_smooth(aes(y = Back_Gross_Profit, col = "back"), se = F) + ylab("") +
#   geom_smooth(aes(y = Total_Gross_Profit, col = "total"), se = F) +
#   geom_smooth(aes(y = .04*Cash_Price, col = "cash"), se = F) 
```

#### If we hope to do anything with this, we will have to extract these trends.

To make this easier, lets create a new df for each day.

### 4. Condensing

```{r}
# Day<- as.data.frame(seq(as.Date("2016-01-01"), as.Date("2020-11-16"), by="days"))
# colnames(Day)<-c("Date")
# temp<- group_by(KDA1, Date) %>%
#   mutate(sum.fgp = sum(Front_Gross_Profit, na.rm = TRUE)) %>%
#   summarise(sum.fgp, .groups = 'drop') %>%
#   unique()
# Day<-left_join(Day, count(group_by(KDA1, Date)), by = "Date")
# Day<-left_join(Day, temp, by = "Date")
# temp<- group_by(KDA1, Date) %>%
#   mutate(sum.bgp= sum(Back_Gross_Profit, na.rm = TRUE)) %>%
#   summarise(sum.bgp, .groups = 'drop') %>%
#   unique()
# Day<-left_join(Day, temp, by = "Date")
# temp<- group_by(KDA1, Date) %>%
#   mutate(sum.tgp = sum(Total_Gross_Profit, na.rm = TRUE)) %>%
#   summarise(sum.tgp, .groups = 'drop') %>%
#   unique()
# Day<-left_join(Day, temp, by = "Date")
# temp<- group_by(KDA1, Date) %>%
#   mutate(sum.cp = sum(Cash_Price, na.rm = TRUE)) %>%
#   summarise(sum.cp, .groups = 'drop') %>%
#   unique()
# Day<-left_join(Day, temp, by = "Date")
```

#### There are 1782 days, with 55 NAs in the mix. Most of them are probably within

that stretch in April of 2019. We will fix them by imputation. First
lets take a look at this new df, now with the number of sales per day
[n]. The missing values will automatically be predicted using a simple
linear model (loess).

```{r}
# summary(Day)
# Day %>%
# ggplot(aes(x = Date)) + 
#   geom_smooth(aes(y =.6*sum.tgp-600, color = "total gross"), 
#               se = F, method = 'loess') +
#   geom_smooth(aes(y = sum.bgp, color = "back gross"), 
#               se = F, method = 'loess') + 
#   geom_smooth(aes(y = .6*sum.fgp + 8700, color = "front gross"), 
#               se = F, method = 'loess') +
#   geom_smooth(aes(y = .15*(sum.cp-183000), color = "cash price"), 
#               se = F, method = 'loess') +
#   geom_smooth(aes(y = 1000*n+5000, color = "n sales"), 
#               se = F, method = 'loess') + ylab("")
```

### 5. Imputation

#### I've chosen to impute the mssing values so we can get a complete df that we

can run through functions and expect a complete output. To do this, we
can use a variety of methods. I've chosen to use predictive mean
matching, which uses a kind of regression that builds relationships
around the mean of each variable. Again, there are many ways to do this
and this might not be the best one. First, we train the algorithm 20
times [m = 20], and we get its best guess at what the missing values
would be according to pmm. We can easily change the method later too.

```{r, include=FALSE}
# m<-'pmm'
# junk<-capture.output(imp <- mice(data = Day, m = 20, method = c("",m,m,m,m,m)))
# Day.full<-complete(imp, 20)
# #The completed df
# Day.full %>%
# ggplot(aes(x = Date)) + 
#   geom_smooth(aes(y =.6*sum.tgp-600, color = "total gross"), 
#               se = F, method = 'loess') +
#   geom_smooth(aes(y = sum.bgp, color = "back gross"), 
#               se = F, method = 'loess') + 
#   geom_smooth(aes(y = .6*sum.fgp + 8700, color = "front gross"), 
#               se = F, method = 'loess') +
#   geom_smooth(aes(y = .15*(sum.cp-183000), color = "cash price"), 
#               se = F, method = 'loess') +
#   geom_smooth(aes(y = 1000*n+5000, color = "n sales"), 
#               se = F, method = 'loess') + ylab("")
```

#### Now, since we have data for each day, we can do some timeries analysis for

all of our metrics. [freq = 365] because we have 365 observations per
year. Remember here, these values are taken from the sum of that value
every day. From, these plots, we really just need to pay attention to
the trend.

```{r}
# b<-decompose(ts(Day.full$sum.bgp, frequency =365))
# plot(b) + title("Back Gross", line = -.5)
# f<-decompose(ts(Day.full$sum.fgp, frequency =365))
# plot(f)+ title("Front Gross", line = -.5)
# t<-decompose(ts(Day.full$sum.tgp, frequency =365))
# plot(t) + title("Total Gross", line = -.5)
# c<-decompose(ts(Day.full$sum.cp, frequency =365))
# plot(c) + title("Cash Price", line = -.5)
# n<-decompose(ts(Day.full$n, frequency =365))
# plot(n) + title("N Sold", line =  -.5)
```

#### Looks nice and informative. Now, lets make sure all our data is in the standard

style so we can easily compare it to other economic trends.

```{r}
#dummy for joining
# time.seq<-seq.Date(from = as.Date("2016-01-01"), to = as.Date("2020-11-16"), by = 'day')
# blank<-as.data.frame(time.seq)
# colnames(blank)<-c("date")
# ## Then the final df
# rogue<- cbind(as.data.frame(Day.full$Date), as.data.frame(b$trend),
#               as.data.frame(f$trend),
#               as.data.frame(t$trend), as.data.frame(c$trend), 
#               as.data.frame(n$trend))
# colnames(rogue)<-c("date", "backg", "frontg", "totalg", "cp", "n")
# rogue<-left_join(blank, rogue)
# #Finally, a very informative plot of our trends so far
# plot(rogue)
```

# Data Sources

```{r, include=FALSE}
# read in json  files
library(rjson)

stock_info <- fromJSON(file = "data/out/stocks_info.json")

fred_info <- fromJSON(file = "data/out/fred_info.json")

trend_info <- fromJSON(file = "data/out/trends_info.json")

get_info <- function(data) {

df <- data.frame()

    for (i in names(data)){
    
        for (j in data[i]) {
            r <- c(key = i, unlist(j))
            }
        df <- rbind(df, r)
    }
    
    names(df) <- c("key", names(data[[1]]))
    
    return(df)
}

info_df <- rbind(get_info(stock_info),
                 get_info(fred_info),
                 get_info(trend_info)) %>%
    dplyr::summarise(name, key, category, updated, `citation`, link)

 # add in best lag and usage per model here
print(info_df)
```

```{r display-info, results='asis'}
for (i in 1:nrow(info_df)) {
  cat("\n\n##[", info_df$name[i], "](", info_df$link[i], ")", sep = "")
  cat("\n_", info_df$key[i], "_", sep = "")
  cat("\n__", info_df$category[i], "__", sep = "")
  cat("\n", info_df$updated[i], sep = "")
  cat("\n", info_df$citation[i], sep = "")
}
```

# References

# Credit

Author: Keaton Markey Version: 1.0 Date Revised: 1/11/2023
