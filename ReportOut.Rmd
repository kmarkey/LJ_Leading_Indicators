---
title: 'Leading Indicators'
subtitle: 'First Report'
author: "Keaton Markey"
date:  "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document:
    latex_engine: xelatex
    toc: true
    number_sections: false
    toc_depth: 6
    highlight: tango
  html_document:
    toc: yes
urlcolor: blue
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup}
library(here)
library(ggplot2)
library(stringr)
library(lubridate)
library(dplyr)
library(logger)
library(here)
library(scales)
library(tidyr)
library(readr)
```

# Introduction:

### Project Aim:

Find external indicators that provide reliable insight into previous
business performance of LJ, and use those indicators to predict future
business performance.

### Project Overview:

# Methods:

## 1. Digest & Prepare Data

Raw sales data, cleaned and summarized by month. Potential indicators
were grouped by data source and summarized by month. They are refreshed
each month.

## 2. Find Leading Indicators

To find potential candidates, I consulted sources used by other similar projects, current
market research, and suggestions. Leading indicators were selected from a
pool of stocks closures, volumes, economic measures, Google Trends information, and some internal
appointment and website data. Finding and extracting good predictors of business performance is also a necessary
step to create accurate and reliable predictions.

A "good" predictor of business performance can mean a few things, and does mean a few things in the scope of this project:
  -   On one hand, a good predictor may mean any set of data that is highly correlated with business performance
  -   On the contrary, a good predictor may just be a set of data that is important for a model to predict business performance.
  
In other words, the leading indicators in this project are identified by both-readable and more complex mathematical methods. Some are better interpreted by people, and some are better interpreted by one of the few algorithms used to predict business performance. Where applicable, each indicator is given a score based on their importance to a model and correlation to business performance which you can view in the app.

Identifying leading indicators can be translated to a machine learning
problem called *feature selection*. The feature selection processes used in this project sometimes combine simple statistics with machine learning methods.

After compiling a list of potential candidates, candidates that met the
following criteria were excluded during the feature selection process:

-   Updated less frequently than every month
-   Not released before the 5th of the month
-   Not available for the duration of available data (2010-present)

Candidates were then modified for the appropriate prediction interval (originally six months). Each indicator and it's modified subsidiaries were grouped together, and the single feature with the highest correlation was passed on as a _feature_.

During the prediction process the following machine learning techniques were employed to determine which indicators were important to each model:

-   Lasso L1 Regularization
-   Gini Impurity pruning
-   Permutation Feature Importance
-   Integrated Gradients

These methods are further described in the next section in conjuction with their respective model.

## 3. Create Predictions

After identifying candidates, each of the following models was used in combination with
feature selection techniques so that, when interpreted in concert, they may give diverse yet probable insights into future business performance.

a)  Linear Model
b)  Decision Tree
c)  Random Forest
d)  ARIMA
e)  GRU
f)  LSTM

Each model utilizes different methods to establish a relationship
between the candidates and business performance. Thus, they must be
interpreted differently. Below is a detailed description fo each model, some guidelines for interpretation, and
additional considerations.

### Linear Model

The linear model establishes a linear relationship between each feature
and the output. This is the most simple method and typically very
effective. The model won't be great at recognizing yearly or quarterly recurring trends, but can be easily quantified mathematically and serves as a great basis of understanding and improvement.

Feature selection was performed with repeated 5-fold cross-validation
through Lasso regression (L1 regularization), optimized for mean-squared-error. The leading indicators used in the final version of this model are best described as those with the strongest linear relationship to the target variable. While this model doesn't capture more complex relationships, it discovers each leading indicator with a robust method that can be easily mapped back to the target with a linear function. Feature importance was calculated using the absolute value of the normalized coefficients.

### Decision Tree

A decision tree is another simple method of prediction that is slow-moving and resistant to outliers. By establishing some binary question for each of the inputs, this model combines the answers into a prediction. For example, if the GM stock closes below 34.00 USD, a decision tree model might predict an decrease of 10 in the target variable. This model works poorly for changes on a small scale, but longer-term trends may be able to be detected early.

###### PHOTO 

Feature selection was performed by trimming the depth of the tree with 5-fold cross-validation to optimize mean-squared-error. Despite its simplicity, this model works well to predict future changes. It can also be understood easily and quantified mathematically. The leading indicators used in this model can be interpreted as those with the greatest impact on minimizing the residual error of the model. Only a few leading indicators are selected by this method, and their importances were calculated using the Gini importance.

### Random Forest

A random forest is a collection of hundreds of decision trees, each composed of different features, that each take votes on estimating the target variable. This method is even less sensitive to large changes due to the voting system. This method is extremely effective and uses the best estimates from the best predictors.

The architecture of the model was set using 5-fold cross-validation on maximum tree depth and the minimum  number of samples per leaf. Feature selection was performed by calculating the permutation importance of each feature on a validation set, and running the model again with only features that had higher than mean feature importance. Features important to this model can be described as those that most often contribute to creating effective decision trees. This model is more complex, and while it can be translated into a series of decision trees, its more challenging to quantify a direct relationship between each leading indicator and the target.

### ARIMA

An Auto-Regressive Integrated Moving Average is a classic model of time-series prediction. Instead of using leading indicators, this makes a prediction based on linear relationships to past observations of the target variable (autoregression) and a calculated moving average. The performance of this model depends greatly on seasonality. This model is a benchmark for the success of this project. If the other regression models cannot outperform it, then it may not be beneficial to consider leading indicators at all.

All features in this model are progressively lagged vectors of the target variable, up to 12 months.

### GRU

A Generalized Recurrent Unit (GRU) is a basic type of neural network designed to place weight on past observations. Instead of one-shot optimization, neural networks are "trained" incrementally on hundreds of copies of the data and over time, come to some understanding of the relationship between the leading indicators and the target.

After its first training pass, this model selects the top 10-20 most important features and is trained again using only those. Feature importance is calculated with the [integrated gradients](https://www.tensorflow.org/tutorials/interpretability/integrated_gradients). Because of the nature of this model and the amount of available data, it was heavily throttled by limiting the amount of training data it sees and the total number of parameters. While they usually achieve great scores, these models are difficult to understand, and thus don't offer a simple 1-1 relationship between input and output. Integrated gradients can give an idea of how important each leading indicator may be to the model, but its difficult to determine exactly what that means.

### LSTM

A Long-Short-Term Memory is a deep neural network that is well-suited to the problem of time-series prediction. This is the most complex model used in this project. Being a bit larger than a GRU, this model was also throttled to reduce risks associated with using overpowered models. 

# Results

## Leading Indicators

As discussed, its important to remember that leading indicators are identified by a few methods ranging in interpretability and usability. In general, leading indicators that are identified by simpler, interpretable methods like a correlation coefficient aren't necessarily as important in predicting business performance. The inverse is also true: leading indicators that are highly important to a model such as the LSTM may not be well-understood or useful to people outside of its importance to the model.

Below is a table of all candidates and their importance with respect to each model.

```{r}

cf <- read_csv("data/out/corr_frame.csv", show_col_types = FALSE, col_names = c("name", "correlation"), skip = 1)

l <- read_csv("models/dev/snapshots/2023-09-22/l-imp.csv",
              col_names = c("idx", "lasso", "name"), 
              show_col_types = FALSE)

t <- read_csv("models/dev/snapshots/2023-09-22/t-imp.csv", 
              col_names = c("idx", "decision tree", "name"),
              show_col_types = FALSE)

r <- read_csv("models/dev/snapshots/2023-09-22/r-imp.csv", 
              col_names = c("idx", "random forest", "name"),
              show_col_types = FALSE)

g <- read_csv("models/dev/snapshots/2023-09-22/g-imp.csv", 
              col_names = c("idx", "gru", "name"),
              show_col_types = FALSE)

m <- read_csv("models/dev/snapshots/2023-09-22/m-imp.csv", 
              col_names = c("idx", "lstm", "name"),
              show_col_types = FALSE)

# join all
coyote <- left_join(cf, l, by = "name") %>%
  left_join(t, by = "name") %>%
  left_join(r, by = "name") %>%
  left_join(g, by = "name") %>%
  left_join(m, by = "name") %>%
  dplyr::select(-starts_with("idx")) %>%
  
  # split out name to root and lag
  separate(col = name,
           into = c("key", "lag"),
           sep = "_lag") %>%
  
  mutate(lag = as.numeric(lag))


# read in json  files
library(rjson)

stock_info <- fromJSON(file = "data/out/stocks_info.json")

fred_info <- fromJSON(file = "data/out/fred_info.json")

trend_info <- fromJSON(file = "data/out/trends_info.json")

get_info <- function(data) {

df <- data.frame()

    for (i in names(data)){
    
        for (j in data[i]) {
            r <- c(key = i, unlist(j))
            }
        df <- rbind(df, r)
    }
    
    names(df) <- c("key", names(data[[1]]))
    
    return(df)
}

info_df <- rbind(get_info(stock_info),
                 get_info(fred_info),
                 get_info(trend_info)) %>%
    dplyr::summarise(name, key, category, updated, `citation`, link)

 # add in best lag and usage per model here

complete_info <- left_join(coyote, info_df, by = "key")
```


## Prediction Models

# Discussion

One of the most important concerns when prediction and training machine
learning models is called *overfitting*. This is a phenomenon that
occurs when the model fits the training data very well, but performs
poorly when asked to predict new data. For example, the models
are able to establish relationships that exist in data from the
past, but don't generalize to make accurate predictions about the
future. 

Every kind of model is prone to overfitting, and this project is especially at risk of encountering this problem for two reasons.

The first reason is the shape of the data. We are working with information from around 165 months. More longitudinal data would improve both prediction power and the accuracy of the feature selection process. With hundreds of similar candidates to choose from, it's ideal to have a number of observations (months) that exceeds the total number of candidates. This would require more data: 
  -   Pre-2010 from the shop
  -   Pre-2010 from all external sources
  that may not be accessible or useful for a variety of reasons

Fortunately, the general shape of the data (known as n << p where n is the number of months and p is the total number of candidates before selection) conveniently lets us accomplish both goals of the project by requiring a robust feature selection process that:
  -   reduces colinearity (duplicate information)
  -   decreases overfitting
  -   and presents a nice way to quantify true leading indicators
  
The second reason is the types of models used in the project. As mentioned, a few of the models were heavily simplified to reduce the risk of overfitting. Typically, the more complex the data, the more complex the model. While complicated models can produce extremely accurate predictions on a training set, predictions can be really inaccurate on new data. It's important to balance scores between the training and test set, especially with the more complex models. Cross-validation is a common solution to this problem as well, and has been employed by most of the models.

With so many candidates for leading indicators, more monthly data could dramatically affect the output of feature
selection and model performance tests. Often methods like bootstrapping and sampling are used to increase the number of observations, but with time series data there is little that can be done without disrupting the sequential nature of observations. We could also considered other intervals, such as weekly or bi-weekly, but would likely introduce more variance and restrict candidate selection even further.

Additionally, with all the data the models use, it is possible that there is more economic information, stock trades, or other metric that may prove useful. More features may still be necessary to improve prediction of business performance and reduce overfitting.
In the future, including more monthly observations and more feature candidates is necessary for improving this project.

## NOTE: COVID-19

COVID-19 put a pause on many people's lives, and many businesses. This is a problem for two reasons. First, only two sales occurred in the month of April 2020, which is a big outlier. Normally, sales hover between 200 and 400 per month. Such a big deviation affects machine learning algorithms greatly, and instead of extracting relationships across the whole time period to improve their score, they would focus on the one or two months that contribute to a high or low score. Removing this outlier is an option, but it would disrupt the model's reliance on sequential past data and reduce the chances that the models would be able to predict an event like this again.

The second and more serious issue is how the pandemic disrupted the economy. While it had a direct impact on sales for this business, it also disrupted the business relationship between groups across the world. Many organizations underwent a period of rapid change, where few emerged operating exactly how they did before April 2020.

The challenge with trying to predict business performance during this time is that whatever relationships existed between businesses before the pandemic have been changed or dissolved entirely. Currently, the models are primarily trained on data before the pandemic to predict the business for all future months. Without a way to quantify relationships that emerged after April 2020, the models make the assumption that the landscape of candidates and their relationship to this business has not changed since then.

## Evaluation

Generally, I take a look at how the predicted line compares to actual business performance, and compare their scores including mean squared error and the coefficient of determination

# 

# Figure Appendix

# Indicator Index

```{r, label="Set Bounds", include=FALSE, echo = FALSE}

KDAc <- read_csv("data/sour/KDAc.csv")
here()

# generated on
today <- Sys.Date()

# last month
lmonth <- floor_date(floor_date(today, unit = "month") - 1, unit = "month")

# bounds of 2nd to last month on record
nmonth <- floor_date(floor_date(max(KDAc$date), unit = "month") - 1, unit = "month")
nomonth <- ceiling_date(nmonth, unit = "month") - 1

# bounds of last month on record
pmonth <- nomonth + 1
pqmonth <- ceiling_date(pmonth, unit = "month") - 1

# report would be made on the 1st of
repon <- pqmonth + 1

# report is for the month of
repfor <- paste0(month(pqmonth, label = TRUE, abbr = FALSE), ", ", year(pqmonth))
```

```{r theme set, include = FALSE, echo = FALSE}
pal <- c("#f6aa1c","#08415c","#6b818c","#eee5e9","#ba7ba1","#c28cae","#a52a2a")

blue <- "#114482"
lightblue <- "#146ff8"
llightblue <- "#AFCFFF"
red <- "#a52a2a"
white <- "#FBFFF1"
yellow <- "#F6AA1C"
green <- "#588157"

ljtheme <- function(){
    theme_minimal() %+replace%
    theme(
      panel.grid.major = element_line(linetype = "solid", color = llightblue, linewidth = 0.1),
      panel.grid.minor = element_blank(),
      panel.background = element_rect(fill = white), #light
      panel.border = element_rect(color = lightblue, fill = NA, linetype = "solid", linewidth = 2),
      legend.background = element_rect(fill = white, color = lightblue, linewidth = 1), # legend
      legend.text = element_text(color = blue),
      legend.title = element_text(face = "bold.italic", color = blue),
      legend.position = "bottom",
      legend.key = element_rect(fill = white),
      
      
      strip.text.x = element_text(size = 12, color = white, face = "bold.italic"),
      strip.text.y = element_text(size = 12, color = white, face = "bold.italic"),
      text = element_text(color = white),
      axis.title = element_text(face = "italic", size = 11, color = white), 
      axis.text = element_text(color = white, size = 9), 
      axis.ticks = element_line(color = white, linewidth = .5, lineend = "butt"), 
      axis.ticks.length = unit(.1, "cm"),
      plot.title = element_text(face = "bold", # labels
                              color = white, size = 14, hjust = 0, vjust = 1.5),
      plot.subtitle = element_text(color = white, hjust = 0, vjust = 1.5, face = "italic"),
      plot.caption = element_text(color = white, face = "bold"),
      plot.background = element_rect(fill = blue)
    )
}

# A function factory for getting integer y-axis values.
integer_breaks <- function(n = 5, ...) {
  fxn <- function(x) {
    breaks <- floor(pretty(x, n, ...))
    names(breaks) <- attr(breaks, "labels")
    breaks
  }
  return(fxn)
}

theme_set(ljtheme())
```

## 1. Finding the Features

After processing sales data, all external indicators are fetched from
the following sources and API packages monthly:

-   Quandl
-   AlphaVantage
    -   with the alpha_vantage library
-   Google Trends
    -   with modifications to the pytrends library
-   St. Louis Federal Reserve

A full list of candidates, as well as which predictions they support,
can be found in the Feature Appendix.

Each indicator is lagged at 3, 6, 9,and 12 month intervals to create
multiple candidates. Each candidate is evaluated for correlation to the
target metric. Any candidate that does not meet the correlation
threshold is eliminated as the first pass of feature selection.

Features are further selected for each model independently in the
following ways:

-   Linear Regression

    -   Each remaining feature is passed through a Lasso regularization
        process, which selects features based on the efficacy of linear
        models that contain them using 5-fold cross-validation.

-   Decision Tree

    -   Regularization is applied after training when the tree is pruned
        at a depth optimized by 5-fold cross-validation.

-   

    ## Random Forest

```{r}

# 6 months
# 1 yr
# 5 yr
# all time

new_used <- "NEW"
purchase_lease <- "L"
metric <- "n"
group <- "weekly" # day, quarter and year?

group_unit <- function(group) {
  if (group == "daily") {
    return("day")
  } else return(substr(group, 1, nchar(group) - 2))
}

data <- KDAc %>%

    {if (new_used != "BOTH")
        
        dplyr::filter(., nu == new_used) # this and last month
      else .
        } %>%
    
    {if (purchase_lease != "BOTH")
        
        dplyr::filter(., pl == purchase_lease)
      else .
      } %>%
  
  group_by(!!group := floor_date(date, group_unit(!!group))) %>%
        
  {if (metric == "n")
    dplyr::summarise(., n = n(), date)
    else
      dplyr::summarise(., !!metric := mean(get(metric), na.rm = TRUE), date)}
  
data %>%
  ggplot() +
      geom_line(aes(x = get(group), y = get(metric)), color = blue, linewidth = 2) +
      geom_point(aes(x = get(group), y = get(metric)), color = blue, size = 1.5) +
      geom_line(aes(x = get(group), y = get(metric)), color = lightblue, linewidth = .5) +



  
  labs(x = "",
       y = paste0(metric))

```

```{r new.used movers}
#### rolling average
#https://stackoverflow.com/questions/33769770/use-rollapply-and-zoo-to-calculate-rolling-average-of-a-column-of-variables

# Change in sales for the 5 most common vehicles new/used

# don't really care about latency, we can dynamically create plots based on parameters

new_used <- "NEW"
movers_n <- 10

# biggest movers from last month ??

movers <- KDAc %>%
  
  {if (new_used != "BOTH")
    dplyr::filter(., nu == new_used, date >= nmonth, date <= pqmonth) # this and last month
    else 
      dplyr::filter(., date >= nmonth, date <= pqmonth)
  } %>%
  
  {if (new_used == "USED") 
    dplyr::mutate(., carname = str_c(make, " ", model), .keep = 'unused')
    else # Only by model, not by year
      dplyr::mutate(., carname = str_c(make, " ", model, " ", caryear), .keep = 'unused')} %>%
  
  dplyr::group_by(carname, month = ifelse(month(date) == month(pqmonth), "current", "last")) %>% # change month names
  
  dplyr::summarise(n = n()) %>%
  
  pivot_wider(id_cols = "carname", names_from = "month", values_from = "n", values_fill = 0) %>%
  
  dplyr::mutate(change = current - last) %>%
  ungroup() %>%
  slice_max(n = movers_n, order_by = abs(change), with_ties = FALSE)

movers %>%
  
  ggplot() + 
  
  geom_segment(aes(x = reorder(carname, change), y = last, xend = carname, yend = current, color = change > 0),
               linewidth = 10, lineend = "round") +
  
  geom_point(aes(x = carname, y = current), color = "black", alpha = 0.2, size  = 10) +

  
  geom_segment(aes(x = reorder(carname, change), y = ifelse(change > 0, current + 2, current - 2), xend = carname, yend = last),
               color = "white",
               linewidth = 1.1, lineend = "round", linejoin = "bevel",
               arrow = arrow(length = unit(0.15, "in"), ends = "first")) +
  
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5),
        legend.position = "none") +
  
  scale_color_manual(values = c(red, green)) +
  
  labs(title = paste0("Biggest Movers This Month (", new_used, ")"),
       subtitle = repfor,
       x = "",
       y = "Change In Sales")

```

```{r Last 6 Months Sales}
# n.months must be less than as.numeric(month(pmonth, abbr = FALSE, label = FALSE))
n.months <- 3

years <- 7

metric <- "n"

metric_dict <- c(
  n = "Total Sales",
  front_gross_profit = "Avg. Front Gross Profit",
  back_gross_profit = "Avg. Back Gross Profit",
  total_gross_profit = "Avg. Total Gross Profit",
  cash_price = "Avg. Cash Price"
)

mlabs <- function(x) {
  paste0(month(
    pmonth - months(as.numeric(x)),
    label = TRUE,
    abbr = FALSE
  ))
}

month(pmonth)

# group by distance from current month
mgroups <- KDAc %>%
  
  dplyr::mutate(mgroup = factor(month(pmonth) - month(date), ordered = TRUE),
                theyear = year(date)) %>%
  
  group_by(mgroup, theyear) %>%
  
  {
    if (metric == "n")
      summarise(., y = n(), mgroup, theyear)
    else
      summarise(., y = mean(get(metric), na.rm = TRUE), mgroup, theyear)
  } %>%
  
  distinct() %>%
  
  dplyr::filter(theyear >= year(pmonth) - years + 1,
                as.numeric(as.character(mgroup)) >= 0 && as.numeric(as.character(mgroup)) <= n.months - 1)

mgroups

ggplot() +
  geom_col(
    data = mgroups,
    aes(
      x = rev(mgroup),
      y = y,
      group = theyear,
      fill = theyear
    ),
    width = 0.8,
    position = 'dodge',
  ) +
  
  geom_text(
    data = mgroups,
    aes(
      x = rev(mgroup),
      y = 0,
      group = theyear,
      label = theyear, 
      color = y < 5 
    ),
    position = position_dodge(width = 0.8),
    angle = 90,
    hjust = -0.1
  ) +

  scale_x_discrete(labels = mlabs, limits = rev) +
  
  scale_y_continuous(labels = comma) +
  
  scale_color_manual(values = c("white", "black")) +
  
  labs(y = '',
       x = '') +
  theme(legend.position = 'none')

```

Compare this month to other years

```{r, label="This Month's Rank Among Other Years"}
calmonth <- KDAc %>%
  dplyr::filter(month(date) == month(pmonth), year(date) <= year(pmonth)) %>%
  group_by(theyear = year(date)) %>%
  dplyr::transmute(`Avg. Cash Price` = mean(cash_price, na.rm = TRUE),
                   `Total Gross` = sum(total_gross_profit, na.rm = TRUE),
                   `Back Gross` = sum(back_gross_profit, na.rm = TRUE),
                   `Front Gross` = sum(front_gross_profit, na.rm = TRUE),
                   `New Sales` = sum(ifelse(nu == "NEW", 1, 0)),
                   `Used Sales` = sum(ifelse(nu == "USED", 1, 0)),
                   ) %>%
  distinct() %>%
  
  pivot_longer(!theyear, names_to = "name", values_to = "value") %>%
  group_by(name = factor(name, levels = c("Total Gross", # ordered
                                          "Back Gross",
                                          "Front Gross",
                                          "Avg. Cash Price",
                                          "New Sales",
                                          "Used Sales"), ordered = TRUE)) %>%
  
  mutate(ranks = order(order(value, decreasing = TRUE)), # actual ranks
         rankval = rank(value)) # numeric bar chart value

ggplot(calmonth) +
  geom_bar(aes(x = theyear, y = rankval, fill = rankval),  stat = "identity") + 
  facet_wrap(~name) +
  geom_text(aes(x = theyear, y = rankval/2, label = ranks), color = white) +
  scale_fill_gradient2(midpoint = 3, low = red, mid = llightblue,
                     high = blue) +
  labs(title = paste0("Ranking ", repfor),
       subtitle = paste0("Compared to other '", month(pmonth, label = TRUE, abbr = FALSE), "'s"),
       x = '',
       y = '') +
  theme(legend.position = "none",
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank())
```

```{r, label="People"}
# best salesman by avg cash price? avg fgp? n sales? for this month

salesman_metric <- "number_of_sales" # button!

salesman.n <- 5

salesman_metric_dict <-
  c(n = "Total Sales", fgp_a = "Avg. Front Gross Profit", cp_a = "Avg. Cash Price")

salespeople <- KDAc %>%
  dplyr::filter(date >= pmonth, date <= pqmonth) %>%
  group_by(salesman) %>%
  dplyr::transmute(
    cp_a = mean(cash_price, na.rm = TRUE),
    fgp_a = mean(front_gross_profit, na.rm = TRUE),
    n = n(),
    salesman
  ) %>%
  distinct() %>%
  ungroup() %>%
  slice_max(n = salesman.n, order_by = get(salesman_metric))

salespeople %>%
  ggplot() + geom_bar(aes(x = reorder(salesman, get(salesman_metric)),
                          y = get(salesman_metric)),
                      stat = "identity", fill = lightblue) +
  {
    if (salesman_metric == "n")
      geom_label(aes(
        x = salesman,
        y = get(salesman_metric),
        label = paste0(salesman, "\n", round(get(salesman_metric), digits = 0))
      ),
      size = 3)
  } +
  {
    if (salesman_metric != "n")
      geom_label(aes(
        x = salesman,
        y = get(salesman_metric),
        label = paste0(salesman, "\n$", scales::comma(round(
          get(salesman_metric), digits = 0
        )))
      ),
      size = 3)
  } +
  # geom_label(aes(x = salesman,
  #                y = get(salesman_metric),
  #                label = paste0(salesman, "\n", scales::comma(round(get(salesman_metric), digits = 0)))),
  #            size = 3) +
  
  labs(
    title = paste("Top Salesman By", salesman_metric_dict[salesman_metric]),
    subtitle = repfor,
    y = salesman_metric_dict[salesman_metric],
    x = ""
  ) +
  scale_y_continuous(expand = expansion(mult = c(-0.5, 0.05)),
                     breaks = integer_breaks()) +
  theme(axis.text.x = element_blank(),
        axis.ticks.x = element_blank())
  
# best fi manager by back gross?

fi.n <- 5

fipeople <- KDAc %>%
  dplyr::filter(date >= pmonth, date <= pqmonth) %>%
  group_by(fimanager) %>%
  dplyr::transmute(bgp_a = mean(back_gross_profit, na.rm = TRUE), fimanager) %>%
  distinct() %>%
  ungroup() %>%
  slice_max(n = fi.n, order_by = bgp_a)

fipeople %>%
  ggplot() + geom_bar(aes(x = reorder(fimanager, bgp_a),
                          y = bgp_a),
                      stat = "identity", fill = lightblue) +
  geom_label(aes(
    x = fimanager,
    y = bgp_a,
    label = paste0(fimanager, "\n $", scales::comma(round(bgp_a, digits = 0)))
  ),
  size = 3) +
  
  labs(
    title = paste("Top Finance Manager By Avg. Back Gross Profit"),
    subtitle = repfor,
    y = "Back Gross Profit",
    x = ""
  ) +
  scale_y_continuous(expand = expansion(mult = c(-0.5, 0.05)),
                     labels = comma) +
  theme(axis.text.x = element_blank(),
        axis.ticks.x = element_blank())
```

```{r}
data <- KDAc %>%

  dplyr::filter(date >= pmonth, date <= pqmonth) %>%
  
  group_by(salesman) %>%
  
  # metric
  {
    if (salesman_metric == "number_of_sales")
      
      dplyr::summarise(., metric = n())
    
    else
      
      dplyr::summarise(., metric = mean(get(salesman_metric), na.rm = TRUE))
  } %>%
  
  distinct() %>%
  
  slice_max(n = salesman.n,
            order_by = metric)


data
# plotting
data %>%
  ggplot() + geom_bar(aes(x = reorder(salesman, metric),
                          y = metric),
                      stat = "identity") +
  
  {
    if (salesman_metric == "number_of_sales")
      geom_label(aes(
        x = salesman,
        y = metric,
        label = paste0(salesman, "\n", round(metric, digits = 0))
      ),
      size = 3)
    
    else
      geom_label(
        aes(
        x = salesman,
        y = metric,
        label = paste0(salesman, "\n$", scales::comma(round(metric, digits = 0)))
      ),
      size = 3)
  } +
  # geom_label(aes(x = salesman,
  #                y = get(salesman_metric),
  #                label = paste0(salesman, "\n", scales::comma(round(get(salesman_metric), digits = 0)))),
  #            size = 3) +
  
  labs(y = "",
       x = "") +
  
  scale_y_continuous(expand = expansion(mult = c(-0.3, 0.1)),
                     breaks = integer_breaks()) +
  
  theme(axis.text.x = element_blank(),
        axis.ticks.x = element_blank())
```

# Next Month?

Based on past business performance cycles, months + years

```{r, label="Next Month", eval = FALSE}
# years on right, months on left?
#
nmonth_metric <- "Back Gross"

estimates_for <- paste0(month(nmonth, abbr = FALSE, label = TRUE), ", ", year(nmonth))

# last 2 months, last 2 years
predate <- as.character(c(as.character(rev(month((nmonth) - months(1:2), label = TRUE, abbr = FALSE))), # last 2 months
                          estimates_for,
                          (year(nmonth) - 1):year((nmonth) - years(2)))) # last 2 years

# group 2 insert for next month
thismonthinsert <- list(group = 2, 
                        tframe = estimates_for, 
                        "Avg. Cash Price" = NULL,
                        "Total Gross" = NULL, 
                        "Back Gross" = NULL,
                        "Front Gross" = NULL, 
                        "New Sales" = NULL,
                        "Used Sales" = NULL)

# previous months + years for next month

nmonth_df <- bind_rows(KDAc %>% # last 2 months
                   dplyr::filter(date >= pmonth - months(1), date < pqmonth) %>%
                   dplyr::mutate(group = 1, tframe = as.character(month(date, label = TRUE, abbr = FALSE))),
                 KDAc %>% # last 2 years
                   dplyr::filter(month(date) == month(pmonth) + 1, year(date) >= year(pmonth) - 2, date < pqmonth) %>%
                   dplyr::mutate(group = 3, tframe = as.character(year(date)))) %>%
  group_by(group, tframe) %>%
  summarise(`Avg. Cash Price` = mean(cash_price, na.rm = TRUE),
            `Total Gross` = sum(total_gross_profit, na.rm = TRUE),
            `Back Gross` = sum(back_gross_profit, na.rm = TRUE),
            `Front Gross` = sum(front_gross_profit, na.rm = TRUE),
            `New Sales` = sum(ifelse(nu == "NEW", 1, 0)),
            `Used Sales` = sum(ifelse(nu == "USED", 1, 0))) %>%
  bind_rows(thismonthinsert) %>%
  dplyr::mutate(tframe = factor(tframe, levels = predate, order = TRUE)) %>%
  pivot_longer(cols = -any_of(c("group", "tframe")), names_to = "class", values_to = "value") %>%
  group_by(group, class) %>%
  dplyr::mutate(a = ifelse(group != 2, coef(lm(value ~ as.numeric(tframe)))[2], 0),
                b = ifelse(group != 2, coef(lm(value ~ as.numeric(tframe)))[1], 0))

if (!(nmonth_metric %in% nmonth_df$class)) {
  warning("Enter a valid class")
} 

# prediction function
predrectf <- function(facet) {
  g1 <- predict(lm(value ~ as.numeric(tframe), data = nmonth_df, subset = (group == 1 & class == nmonth_metric)), list(tframe = 3))
  g3 <- predict(lm(value ~ as.numeric(tframe), data = nmonth_df, subset = (group == 3 & class == nmonth_metric)), list(tframe = 3))
  return(c(group1 = g1, group3 = g3))
}

# get bounding box
predrect <- predrectf()

nmonth_df %>%
  filter(class == nmonth_metric) %>%
  ggplot() + 
  geom_point(aes(x = tframe, y = value, fill = factor(group), shape = factor(group)),
             size = 5,
             color = "black",
             inherit.aes = FALSE) +
  scale_shape_manual(values = c(25, 2, 22)) +
  geom_abline(aes(slope = a, intercept = b, color = factor(group)),
             lty = 2, size = 2) +
  theme(axis.text.x = element_text(angle = 15),
        legend.position = "none") +
  geom_rect(aes(xmin = 2.8, xmax = 3.2, ymin = min(predrect), ymax = max(predrect)),  # should always be between 5.5 and 6.5
            stat = "identity", 
            data = nmonth_df %>%
             filter(group == 2), 
            inherit.aes = FALSE, 
            alpha = 0.1,
            color = "black") +
  scale_y_continuous(labels = comma) + 
  coord_cartesian(ylim = c(min(predrect) * 0.7, max(predrect) * 1.3)) +
  labs(title = paste0(nmonth_metric, " Estimate for This Month"),
       subtitle = paste0(month(repon, abbr = FALSE, label = TRUE), ", ", year(repon)),
       x = "",
       y = "Value")
```

# Predictions

```{r}
# read in imaginary df(s)


```

```{r, label="Cumulative Tracker"}

```

### 2. Summary statistics

```{r Summary}
```

#### We have some NA's, or empty spaces, in the data. This could be for a

#### We can use logistic regression to tell us how much the NAs might influence

the data longitudinally. This is probably unnecessary but I'm going to
do it anyway.

#### Based on the models, we noticed a significant trend for all the metrics.

It shouldn't impact analysis too much because we didn't test numeric
variables, but its something to keep in mind.

Generally, we saw more missing Front Gross Profit values and Cash Price
values at the end of the data set, and we saw more missing values for
Back Gross Profit and Total Gross Profit at the beginning of the data
set.

### 3. Plotting

#### Now, lets look at some quick plots of the data.

```{r}
# ggplot(KDA1, aes(Date, Front_Gross_Profit)) + geom_point() + geom_smooth()
# ggplot(KDA1, aes(Date, Back_Gross_Profit)) + geom_point() + geom_smooth()
# ggplot(KDA1, aes(Date, Total_Gross_Profit)) + geom_point() + geom_smooth()
# ggplot(KDA1, aes(Date, Cash_Price)) + geom_point() + geom_smooth()
```

#### 

```{r}
# ggplot(KDA1, aes(x = Date)) + geom_smooth(aes(y = Front_Gross_Profit, col = "front"), se = F) +
#   geom_smooth(aes(y = Back_Gross_Profit, col = "back"), se = F) + ylab("") +
#   geom_smooth(aes(y = Total_Gross_Profit, col = "total"), se = F) +
#   geom_smooth(aes(y = .04*Cash_Price, col = "cash"), se = F) 
```

#### If we hope to do anything with this, we will have to extract these trends.

To make this easier, lets create a new df for each day.

### 4. Condensing

```{r}
# Day<- as.data.frame(seq(as.Date("2016-01-01"), as.Date("2020-11-16"), by="days"))
# colnames(Day)<-c("Date")
# temp<- group_by(KDA1, Date) %>%
#   mutate(sum.fgp = sum(Front_Gross_Profit, na.rm = TRUE)) %>%
#   summarise(sum.fgp, .groups = 'drop') %>%
#   unique()
# Day<-left_join(Day, count(group_by(KDA1, Date)), by = "Date")
# Day<-left_join(Day, temp, by = "Date")
# temp<- group_by(KDA1, Date) %>%
#   mutate(sum.bgp= sum(Back_Gross_Profit, na.rm = TRUE)) %>%
#   summarise(sum.bgp, .groups = 'drop') %>%
#   unique()
# Day<-left_join(Day, temp, by = "Date")
# temp<- group_by(KDA1, Date) %>%
#   mutate(sum.tgp = sum(Total_Gross_Profit, na.rm = TRUE)) %>%
#   summarise(sum.tgp, .groups = 'drop') %>%
#   unique()
# Day<-left_join(Day, temp, by = "Date")
# temp<- group_by(KDA1, Date) %>%
#   mutate(sum.cp = sum(Cash_Price, na.rm = TRUE)) %>%
#   summarise(sum.cp, .groups = 'drop') %>%
#   unique()
# Day<-left_join(Day, temp, by = "Date")
```

#### There are 1782 days, with 55 NAs in the mix. Most of them are probably within

that stretch in April of 2019. We will fix them by imputation. First
lets take a look at this new df, now with the number of sales per day
[n]. The missing values will automatically be predicted using a simple
linear model (loess).

```{r}
# summary(Day)
# Day %>%
# ggplot(aes(x = Date)) + 
#   geom_smooth(aes(y =.6*sum.tgp-600, color = "total gross"), 
#               se = F, method = 'loess') +
#   geom_smooth(aes(y = sum.bgp, color = "back gross"), 
#               se = F, method = 'loess') + 
#   geom_smooth(aes(y = .6*sum.fgp + 8700, color = "front gross"), 
#               se = F, method = 'loess') +
#   geom_smooth(aes(y = .15*(sum.cp-183000), color = "cash price"), 
#               se = F, method = 'loess') +
#   geom_smooth(aes(y = 1000*n+5000, color = "n sales"), 
#               se = F, method = 'loess') + ylab("")
```

### 5. Imputation

#### I've chosen to impute the mssing values so we can get a complete df that we

can run through functions and expect a complete output. To do this, we
can use a variety of methods. I've chosen to use predictive mean
matching, which uses a kind of regression that builds relationships
around the mean of each variable. Again, there are many ways to do this
and this might not be the best one. First, we train the algorithm 20
times [m = 20], and we get its best guess at what the missing values
would be according to pmm. We can easily change the method later too.

```{r, include=FALSE}
# m<-'pmm'
# junk<-capture.output(imp <- mice(data = Day, m = 20, method = c("",m,m,m,m,m)))
# Day.full<-complete(imp, 20)
# #The completed df
# Day.full %>%
# ggplot(aes(x = Date)) + 
#   geom_smooth(aes(y =.6*sum.tgp-600, color = "total gross"), 
#               se = F, method = 'loess') +
#   geom_smooth(aes(y = sum.bgp, color = "back gross"), 
#               se = F, method = 'loess') + 
#   geom_smooth(aes(y = .6*sum.fgp + 8700, color = "front gross"), 
#               se = F, method = 'loess') +
#   geom_smooth(aes(y = .15*(sum.cp-183000), color = "cash price"), 
#               se = F, method = 'loess') +
#   geom_smooth(aes(y = 1000*n+5000, color = "n sales"), 
#               se = F, method = 'loess') + ylab("")
```

#### Now, since we have data for each day, we can do some timeries analysis for

all of our metrics. [freq = 365] because we have 365 observations per
year. Remember here, these values are taken from the sum of that value
every day. From, these plots, we really just need to pay attention to
the trend.

```{r}
# b<-decompose(ts(Day.full$sum.bgp, frequency =365))
# plot(b) + title("Back Gross", line = -.5)
# f<-decompose(ts(Day.full$sum.fgp, frequency =365))
# plot(f)+ title("Front Gross", line = -.5)
# t<-decompose(ts(Day.full$sum.tgp, frequency =365))
# plot(t) + title("Total Gross", line = -.5)
# c<-decompose(ts(Day.full$sum.cp, frequency =365))
# plot(c) + title("Cash Price", line = -.5)
# n<-decompose(ts(Day.full$n, frequency =365))
# plot(n) + title("N Sold", line =  -.5)
```

#### Looks nice and informative. Now, lets make sure all our data is in the standard

style so we can easily compare it to other economic trends.

```{r}
#dummy for joining
# time.seq<-seq.Date(from = as.Date("2016-01-01"), to = as.Date("2020-11-16"), by = 'day')
# blank<-as.data.frame(time.seq)
# colnames(blank)<-c("date")
# ## Then the final df
# rogue<- cbind(as.data.frame(Day.full$Date), as.data.frame(b$trend),
#               as.data.frame(f$trend),
#               as.data.frame(t$trend), as.data.frame(c$trend), 
#               as.data.frame(n$trend))
# colnames(rogue)<-c("date", "backg", "frontg", "totalg", "cp", "n")
# rogue<-left_join(blank, rogue)
# #Finally, a very informative plot of our trends so far
# plot(rogue)
```

# Data Sources

```{r, include=FALSE}



```

```{r display-info, results='asis'}
for (i in 1:nrow(info_df)) {
  cat("\n\n##[", info_df$name[i], "](", info_df$link[i], ")", sep = "")
  cat("\n_", info_df$key[i], "_", sep = "")
  cat("\n__", info_df$category[i], "__", sep = "")
  cat("\n", info_df$updated[i], sep = "")
  cat("\n", info_df$citation[i], sep = "")
}
```

# References

# Credit

Author: Keaton Markey Version: 1.0 Date Revised: 1/11/2023
