{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8714181",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1263615d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 39)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"data/csv/features.csv\")\n",
    "len(data), len(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fdbc481",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = data.drop(columns = ['n']), data['n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a61a7f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "753f0cab",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "test_size=0 should be either positive and smaller than the number of samples 50 or a float in the (0, 1) range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-eec48e2042d3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m69\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2419\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2420\u001b[0m     n_train, n_test = _validate_shuffle_split(\n\u001b[1;32m-> 2421\u001b[1;33m         \u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault_test_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.25\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2422\u001b[0m     )\n\u001b[0;32m   2423\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36m_validate_shuffle_split\u001b[1;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[0;32m   2044\u001b[0m             \u001b[1;34m\"test_size={0} should be either positive and smaller\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2045\u001b[0m             \u001b[1;34m\" than the number of samples {1} or a float in the \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2046\u001b[1;33m             \u001b[1;34m\"(0, 1) range\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2047\u001b[0m         )\n\u001b[0;32m   2048\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: test_size=0 should be either positive and smaller than the number of samples 50 or a float in the (0, 1) range"
     ]
    }
   ],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0, random_state=69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c048dcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', Lasso())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f053e198",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = GridSearchCV(pipeline,\n",
    "                      {'model__alpha':np.arange(0.1, 3, 0.1)},\n",
    "                      cv = 5,\n",
    "                      scoring = 'neg_mean_squared_error',\n",
    "                      verbose = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a16172c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 29 candidates, totalling 145 fits\n",
      "[CV 1/5] END .............model__alpha=0.1;, score=-16024.734 total time=   0.0s\n",
      "[CV 2/5] END ..............model__alpha=0.1;, score=-8290.126 total time=   0.0s\n",
      "[CV 3/5] END ..............model__alpha=0.1;, score=-6755.527 total time=   0.0s\n",
      "[CV 4/5] END ..............model__alpha=0.1;, score=-9759.530 total time=   0.0s\n",
      "[CV 5/5] END .............model__alpha=0.1;, score=-48851.942 total time=   0.0s\n",
      "[CV 1/5] END .............model__alpha=0.2;, score=-13521.804 total time=   0.0s\n",
      "[CV 2/5] END ..............model__alpha=0.2;, score=-6185.056 total time=   0.0s\n",
      "[CV 3/5] END ..............model__alpha=0.2;, score=-5184.841 total time=   0.0s\n",
      "[CV 4/5] END ..............model__alpha=0.2;, score=-6274.322 total time=   0.0s\n",
      "[CV 5/5] END .............model__alpha=0.2;, score=-33641.291 total time=   0.0s\n",
      "[CV 1/5] END model__alpha=0.30000000000000004;, score=-13393.740 total time=   0.0s\n",
      "[CV 2/5] END model__alpha=0.30000000000000004;, score=-5120.958 total time=   0.0s\n",
      "[CV 3/5] END model__alpha=0.30000000000000004;, score=-4363.693 total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\keato\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.763e+03, tolerance: 8.321e+00\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
      "C:\\Users\\keato\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.464e+03, tolerance: 7.845e+00\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
      "C:\\Users\\keato\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.495e+03, tolerance: 6.286e+00\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
      "C:\\Users\\keato\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.798e+03, tolerance: 6.308e+00\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
      "C:\\Users\\keato\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.534e+03, tolerance: 7.824e+00\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
      "C:\\Users\\keato\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.097e+01, tolerance: 8.321e+00\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
      "C:\\Users\\keato\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.065e+03, tolerance: 7.845e+00\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
      "C:\\Users\\keato\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.396e+02, tolerance: 6.286e+00\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
      "C:\\Users\\keato\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.507e+01, tolerance: 6.308e+00\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
      "C:\\Users\\keato\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.623e+02, tolerance: 7.824e+00\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
      "C:\\Users\\keato\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.899e+00, tolerance: 8.321e+00\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
      "C:\\Users\\keato\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.246e+01, tolerance: 7.845e+00\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
      "C:\\Users\\keato\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.058e+02, tolerance: 6.286e+00\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5] END model__alpha=0.30000000000000004;, score=-5130.901 total time=   0.0s\n",
      "[CV 5/5] END model__alpha=0.30000000000000004;, score=-16018.705 total time=   0.0s\n",
      "[CV 1/5] END .............model__alpha=0.4;, score=-12914.326 total time=   0.0s\n",
      "[CV 2/5] END ..............model__alpha=0.4;, score=-4657.107 total time=   0.0s\n",
      "[CV 3/5] END ..............model__alpha=0.4;, score=-4353.306 total time=   0.0s\n",
      "[CV 4/5] END ..............model__alpha=0.4;, score=-4590.406 total time=   0.0s\n",
      "[CV 5/5] END ..............model__alpha=0.4;, score=-7797.566 total time=   0.0s\n",
      "[CV 1/5] END .............model__alpha=0.5;, score=-11382.001 total time=   0.0s\n",
      "[CV 2/5] END ..............model__alpha=0.5;, score=-4110.762 total time=   0.0s\n",
      "[CV 3/5] END ..............model__alpha=0.5;, score=-4310.199 total time=   0.0s\n",
      "[CV 4/5] END ..............model__alpha=0.5;, score=-4220.817 total time=   0.0s\n",
      "[CV 5/5] END ..............model__alpha=0.5;, score=-6693.752 total time=   0.0s\n",
      "[CV 1/5] END ..............model__alpha=0.6;, score=-9767.309 total time=   0.0s\n",
      "[CV 2/5] END ..............model__alpha=0.6;, score=-3638.609 total time=   0.0s\n",
      "[CV 3/5] END ..............model__alpha=0.6;, score=-4136.718 total time=   0.0s\n",
      "[CV 4/5] END ..............model__alpha=0.6;, score=-3917.222 total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\keato\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.665e+01, tolerance: 7.824e+00\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
      "C:\\Users\\keato\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.532e+01, tolerance: 7.845e+00\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
      "C:\\Users\\keato\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.009e+01, tolerance: 6.286e+00\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
      "C:\\Users\\keato\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.010e+01, tolerance: 7.824e+00\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END ..............model__alpha=0.6;, score=-5856.686 total time=   0.0s\n",
      "[CV 1/5] END model__alpha=0.7000000000000001;, score=-8428.377 total time=   0.0s\n",
      "[CV 2/5] END model__alpha=0.7000000000000001;, score=-3000.334 total time=   0.0s\n",
      "[CV 3/5] END model__alpha=0.7000000000000001;, score=-4024.018 total time=   0.0s\n",
      "[CV 4/5] END model__alpha=0.7000000000000001;, score=-3687.251 total time=   0.0s\n",
      "[CV 5/5] END model__alpha=0.7000000000000001;, score=-5072.754 total time=   0.0s\n",
      "[CV 1/5] END ..............model__alpha=0.8;, score=-7645.732 total time=   0.0s\n",
      "[CV 2/5] END ..............model__alpha=0.8;, score=-2476.087 total time=   0.0s\n",
      "[CV 3/5] END ..............model__alpha=0.8;, score=-3912.416 total time=   0.0s\n",
      "[CV 4/5] END ..............model__alpha=0.8;, score=-3542.100 total time=   0.0s\n",
      "[CV 5/5] END ..............model__alpha=0.8;, score=-4865.568 total time=   0.0s\n",
      "[CV 1/5] END ..............model__alpha=0.9;, score=-7021.044 total time=   0.0s\n",
      "[CV 2/5] END ..............model__alpha=0.9;, score=-2065.317 total time=   0.0s\n",
      "[CV 3/5] END ..............model__alpha=0.9;, score=-3802.716 total time=   0.0s\n",
      "[CV 4/5] END ..............model__alpha=0.9;, score=-3455.764 total time=   0.0s\n",
      "[CV 5/5] END ..............model__alpha=0.9;, score=-4668.245 total time=   0.0s\n",
      "[CV 1/5] END ..............model__alpha=1.0;, score=-6446.811 total time=   0.0s\n",
      "[CV 2/5] END ..............model__alpha=1.0;, score=-1767.858 total time=   0.0s\n",
      "[CV 3/5] END ..............model__alpha=1.0;, score=-3687.143 total time=   0.0s\n",
      "[CV 4/5] END ..............model__alpha=1.0;, score=-3413.197 total time=   0.0s\n",
      "[CV 5/5] END ..............model__alpha=1.0;, score=-4592.367 total time=   0.0s\n",
      "[CV 1/5] END ..............model__alpha=1.1;, score=-6008.340 total time=   0.0s\n",
      "[CV 2/5] END ..............model__alpha=1.1;, score=-1645.194 total time=   0.0s\n",
      "[CV 3/5] END ..............model__alpha=1.1;, score=-3574.582 total time=   0.0s\n",
      "[CV 4/5] END ..............model__alpha=1.1;, score=-3386.060 total time=   0.0s\n",
      "[CV 5/5] END ..............model__alpha=1.1;, score=-4452.179 total time=   0.0s\n",
      "[CV 1/5] END model__alpha=1.2000000000000002;, score=-5644.419 total time=   0.0s\n",
      "[CV 2/5] END model__alpha=1.2000000000000002;, score=-1590.917 total time=   0.0s\n",
      "[CV 3/5] END model__alpha=1.2000000000000002;, score=-3470.561 total time=   0.0s\n",
      "[CV 4/5] END model__alpha=1.2000000000000002;, score=-3359.961 total time=   0.0s\n",
      "[CV 5/5] END model__alpha=1.2000000000000002;, score=-4291.634 total time=   0.0s\n",
      "[CV 1/5] END model__alpha=1.3000000000000003;, score=-5422.639 total time=   0.0s\n",
      "[CV 2/5] END model__alpha=1.3000000000000003;, score=-1548.361 total time=   0.0s\n",
      "[CV 3/5] END model__alpha=1.3000000000000003;, score=-3375.944 total time=   0.0s\n",
      "[CV 4/5] END model__alpha=1.3000000000000003;, score=-3334.877 total time=   0.0s\n",
      "[CV 5/5] END model__alpha=1.3000000000000003;, score=-4137.140 total time=   0.0s\n",
      "[CV 1/5] END model__alpha=1.4000000000000001;, score=-5238.003 total time=   0.0s\n",
      "[CV 2/5] END model__alpha=1.4000000000000001;, score=-1508.250 total time=   0.0s\n",
      "[CV 3/5] END model__alpha=1.4000000000000001;, score=-3285.327 total time=   0.0s\n",
      "[CV 4/5] END model__alpha=1.4000000000000001;, score=-3310.694 total time=   0.0s\n",
      "[CV 5/5] END model__alpha=1.4000000000000001;, score=-4204.714 total time=   0.0s\n",
      "[CV 1/5] END model__alpha=1.5000000000000002;, score=-5098.229 total time=   0.0s\n",
      "[CV 2/5] END model__alpha=1.5000000000000002;, score=-1463.039 total time=   0.0s\n",
      "[CV 3/5] END model__alpha=1.5000000000000002;, score=-3193.364 total time=   0.0s\n",
      "[CV 4/5] END model__alpha=1.5000000000000002;, score=-3287.290 total time=   0.0s\n",
      "[CV 5/5] END model__alpha=1.5000000000000002;, score=-4038.307 total time=   0.0s\n",
      "[CV 1/5] END ..............model__alpha=1.6;, score=-5014.772 total time=   0.0s\n",
      "[CV 2/5] END ..............model__alpha=1.6;, score=-1408.308 total time=   0.0s\n",
      "[CV 3/5] END ..............model__alpha=1.6;, score=-3103.029 total time=   0.0s\n",
      "[CV 4/5] END ..............model__alpha=1.6;, score=-3264.231 total time=   0.0s\n",
      "[CV 5/5] END ..............model__alpha=1.6;, score=-3893.124 total time=   0.0s\n",
      "[CV 1/5] END model__alpha=1.7000000000000002;, score=-4934.617 total time=   0.0s\n",
      "[CV 2/5] END model__alpha=1.7000000000000002;, score=-1357.566 total time=   0.0s\n",
      "[CV 3/5] END model__alpha=1.7000000000000002;, score=-3014.574 total time=   0.0s\n",
      "[CV 4/5] END model__alpha=1.7000000000000002;, score=-3241.652 total time=   0.0s\n",
      "[CV 5/5] END model__alpha=1.7000000000000002;, score=-3751.597 total time=   0.0s\n",
      "[CV 1/5] END model__alpha=1.8000000000000003;, score=-4855.801 total time=   0.0s\n",
      "[CV 2/5] END model__alpha=1.8000000000000003;, score=-1303.857 total time=   0.0s\n",
      "[CV 3/5] END model__alpha=1.8000000000000003;, score=-2929.677 total time=   0.0s\n",
      "[CV 4/5] END model__alpha=1.8000000000000003;, score=-3219.516 total time=   0.0s\n",
      "[CV 5/5] END model__alpha=1.8000000000000003;, score=-3684.661 total time=   0.0s\n",
      "[CV 1/5] END model__alpha=1.9000000000000001;, score=-4778.327 total time=   0.0s\n",
      "[CV 2/5] END model__alpha=1.9000000000000001;, score=-1254.959 total time=   0.0s\n",
      "[CV 3/5] END model__alpha=1.9000000000000001;, score=-2849.221 total time=   0.0s\n",
      "[CV 4/5] END model__alpha=1.9000000000000001;, score=-3197.759 total time=   0.0s\n",
      "[CV 5/5] END model__alpha=1.9000000000000001;, score=-3651.267 total time=   0.0s\n",
      "[CV 1/5] END ..............model__alpha=2.0;, score=-4689.733 total time=   0.0s\n",
      "[CV 2/5] END ..............model__alpha=2.0;, score=-1210.916 total time=   0.0s\n",
      "[CV 3/5] END ..............model__alpha=2.0;, score=-2818.823 total time=   0.0s\n",
      "[CV 4/5] END ..............model__alpha=2.0;, score=-3176.364 total time=   0.0s\n",
      "[CV 5/5] END ..............model__alpha=2.0;, score=-3618.635 total time=   0.0s\n",
      "[CV 1/5] END ..............model__alpha=2.1;, score=-4590.761 total time=   0.0s\n",
      "[CV 2/5] END ..............model__alpha=2.1;, score=-1171.270 total time=   0.0s\n",
      "[CV 3/5] END ..............model__alpha=2.1;, score=-2815.987 total time=   0.0s\n",
      "[CV 4/5] END ..............model__alpha=2.1;, score=-3156.462 total time=   0.0s\n",
      "[CV 5/5] END ..............model__alpha=2.1;, score=-3582.287 total time=   0.0s\n",
      "[CV 1/5] END ..............model__alpha=2.2;, score=-4495.590 total time=   0.0s\n",
      "[CV 2/5] END ..............model__alpha=2.2;, score=-1148.980 total time=   0.0s\n",
      "[CV 3/5] END ..............model__alpha=2.2;, score=-2813.173 total time=   0.0s\n",
      "[CV 4/5] END ..............model__alpha=2.2;, score=-3139.008 total time=   0.0s\n",
      "[CV 5/5] END ..............model__alpha=2.2;, score=-3497.730 total time=   0.0s\n",
      "[CV 1/5] END model__alpha=2.3000000000000003;, score=-4405.166 total time=   0.0s\n",
      "[CV 2/5] END model__alpha=2.3000000000000003;, score=-1127.583 total time=   0.0s\n",
      "[CV 3/5] END model__alpha=2.3000000000000003;, score=-2810.383 total time=   0.0s\n",
      "[CV 4/5] END model__alpha=2.3000000000000003;, score=-3121.892 total time=   0.0s\n",
      "[CV 5/5] END model__alpha=2.3000000000000003;, score=-3475.707 total time=   0.0s\n",
      "[CV 1/5] END model__alpha=2.4000000000000004;, score=-4327.166 total time=   0.0s\n",
      "[CV 2/5] END model__alpha=2.4000000000000004;, score=-1106.819 total time=   0.0s\n",
      "[CV 3/5] END model__alpha=2.4000000000000004;, score=-2807.611 total time=   0.0s\n",
      "[CV 4/5] END model__alpha=2.4000000000000004;, score=-3105.101 total time=   0.0s\n",
      "[CV 5/5] END model__alpha=2.4000000000000004;, score=-3451.518 total time=   0.0s\n",
      "[CV 1/5] END model__alpha=2.5000000000000004;, score=-4255.946 total time=   0.0s\n",
      "[CV 2/5] END model__alpha=2.5000000000000004;, score=-1086.590 total time=   0.0s\n",
      "[CV 3/5] END model__alpha=2.5000000000000004;, score=-2804.863 total time=   0.0s\n",
      "[CV 4/5] END model__alpha=2.5000000000000004;, score=-3088.543 total time=   0.0s\n",
      "[CV 5/5] END model__alpha=2.5000000000000004;, score=-3428.246 total time=   0.0s\n",
      "[CV 1/5] END ..............model__alpha=2.6;, score=-4185.585 total time=   0.0s\n",
      "[CV 2/5] END ..............model__alpha=2.6;, score=-1066.894 total time=   0.0s\n",
      "[CV 3/5] END ..............model__alpha=2.6;, score=-2802.139 total time=   0.0s\n",
      "[CV 4/5] END ..............model__alpha=2.6;, score=-3072.325 total time=   0.0s\n",
      "[CV 5/5] END ..............model__alpha=2.6;, score=-3405.877 total time=   0.0s\n",
      "[CV 1/5] END ..............model__alpha=2.7;, score=-4116.003 total time=   0.0s\n",
      "[CV 2/5] END ..............model__alpha=2.7;, score=-1047.735 total time=   0.0s\n",
      "[CV 3/5] END ..............model__alpha=2.7;, score=-2799.437 total time=   0.0s\n",
      "[CV 4/5] END ..............model__alpha=2.7;, score=-3056.475 total time=   0.0s\n",
      "[CV 5/5] END ..............model__alpha=2.7;, score=-3383.565 total time=   0.0s\n",
      "[CV 1/5] END model__alpha=2.8000000000000003;, score=-4047.143 total time=   0.0s\n",
      "[CV 2/5] END model__alpha=2.8000000000000003;, score=-1029.111 total time=   0.0s\n",
      "[CV 3/5] END model__alpha=2.8000000000000003;, score=-2796.754 total time=   0.0s\n",
      "[CV 4/5] END model__alpha=2.8000000000000003;, score=-3040.944 total time=   0.0s\n",
      "[CV 5/5] END model__alpha=2.8000000000000003;, score=-3334.586 total time=   0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END model__alpha=2.9000000000000004;, score=-3979.163 total time=   0.0s\n",
      "[CV 2/5] END model__alpha=2.9000000000000004;, score=-1011.019 total time=   0.0s\n",
      "[CV 3/5] END model__alpha=2.9000000000000004;, score=-2794.097 total time=   0.0s\n",
      "[CV 4/5] END model__alpha=2.9000000000000004;, score=-3025.427 total time=   0.0s\n",
      "[CV 5/5] END model__alpha=2.9000000000000004;, score=-3286.095 total time=   0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                                       ('model', Lasso())]),\n",
       "             param_grid={'model__alpha': array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. , 1.1, 1.2, 1.3,\n",
       "       1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2. , 2.1, 2.2, 2.3, 2.4, 2.5, 2.6,\n",
       "       2.7, 2.8, 2.9])},\n",
       "             scoring='neg_mean_squared_error', verbose=3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f8596c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model__alpha': 2.9000000000000004}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cc7ee3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef = search.best_estimator_[1].coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f13962b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>oil_lag6</th>\n",
       "      <th>oil_lag12</th>\n",
       "      <th>ngf2_lag6</th>\n",
       "      <th>sfrm_lag12</th>\n",
       "      <th>durable_lag12</th>\n",
       "      <th>altsales_lag3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50.51</td>\n",
       "      <td>44.83</td>\n",
       "      <td>22</td>\n",
       "      <td>2.86</td>\n",
       "      <td>224061</td>\n",
       "      <td>17.818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>47.01</td>\n",
       "      <td>56.83</td>\n",
       "      <td>21</td>\n",
       "      <td>2.99</td>\n",
       "      <td>218966</td>\n",
       "      <td>17.863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>43.58</td>\n",
       "      <td>51.06</td>\n",
       "      <td>62</td>\n",
       "      <td>2.92</td>\n",
       "      <td>227573</td>\n",
       "      <td>17.081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>43.66</td>\n",
       "      <td>62.21</td>\n",
       "      <td>64</td>\n",
       "      <td>2.85</td>\n",
       "      <td>224339</td>\n",
       "      <td>17.610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38.93</td>\n",
       "      <td>60.47</td>\n",
       "      <td>27</td>\n",
       "      <td>2.90</td>\n",
       "      <td>217332</td>\n",
       "      <td>17.640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>31.27</td>\n",
       "      <td>58.79</td>\n",
       "      <td>103</td>\n",
       "      <td>2.98</td>\n",
       "      <td>232766</td>\n",
       "      <td>16.826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>29.11</td>\n",
       "      <td>50.51</td>\n",
       "      <td>56</td>\n",
       "      <td>2.95</td>\n",
       "      <td>230189</td>\n",
       "      <td>17.247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>30.13</td>\n",
       "      <td>47.01</td>\n",
       "      <td>37</td>\n",
       "      <td>2.90</td>\n",
       "      <td>222647</td>\n",
       "      <td>17.288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>34.33</td>\n",
       "      <td>43.58</td>\n",
       "      <td>57</td>\n",
       "      <td>2.91</td>\n",
       "      <td>218738</td>\n",
       "      <td>17.334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>42.70</td>\n",
       "      <td>43.66</td>\n",
       "      <td>82</td>\n",
       "      <td>2.89</td>\n",
       "      <td>226032</td>\n",
       "      <td>17.744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>45.15</td>\n",
       "      <td>38.93</td>\n",
       "      <td>180</td>\n",
       "      <td>3.01</td>\n",
       "      <td>223223</td>\n",
       "      <td>17.541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>46.27</td>\n",
       "      <td>31.27</td>\n",
       "      <td>30</td>\n",
       "      <td>3.08</td>\n",
       "      <td>217474</td>\n",
       "      <td>17.574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>38.97</td>\n",
       "      <td>29.11</td>\n",
       "      <td>38</td>\n",
       "      <td>2.90</td>\n",
       "      <td>226905</td>\n",
       "      <td>17.606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>43.91</td>\n",
       "      <td>30.13</td>\n",
       "      <td>79</td>\n",
       "      <td>2.79</td>\n",
       "      <td>217371</td>\n",
       "      <td>17.411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>44.63</td>\n",
       "      <td>34.33</td>\n",
       "      <td>75</td>\n",
       "      <td>2.90</td>\n",
       "      <td>216899</td>\n",
       "      <td>17.906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>45.83</td>\n",
       "      <td>42.70</td>\n",
       "      <td>84</td>\n",
       "      <td>2.86</td>\n",
       "      <td>228585</td>\n",
       "      <td>17.270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>44.80</td>\n",
       "      <td>45.15</td>\n",
       "      <td>28</td>\n",
       "      <td>2.87</td>\n",
       "      <td>221991</td>\n",
       "      <td>17.446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>53.30</td>\n",
       "      <td>46.27</td>\n",
       "      <td>193</td>\n",
       "      <td>2.70</td>\n",
       "      <td>209356</td>\n",
       "      <td>16.604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>52.19</td>\n",
       "      <td>38.97</td>\n",
       "      <td>44</td>\n",
       "      <td>2.78</td>\n",
       "      <td>217997</td>\n",
       "      <td>16.798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>53.40</td>\n",
       "      <td>43.91</td>\n",
       "      <td>263</td>\n",
       "      <td>2.75</td>\n",
       "      <td>219140</td>\n",
       "      <td>16.751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>50.43</td>\n",
       "      <td>44.63</td>\n",
       "      <td>27</td>\n",
       "      <td>2.81</td>\n",
       "      <td>216058</td>\n",
       "      <td>16.816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>49.33</td>\n",
       "      <td>45.83</td>\n",
       "      <td>40</td>\n",
       "      <td>2.84</td>\n",
       "      <td>234217</td>\n",
       "      <td>16.844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>48.69</td>\n",
       "      <td>44.80</td>\n",
       "      <td>71</td>\n",
       "      <td>3.12</td>\n",
       "      <td>216703</td>\n",
       "      <td>16.642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>45.63</td>\n",
       "      <td>53.30</td>\n",
       "      <td>57</td>\n",
       "      <td>3.30</td>\n",
       "      <td>218450</td>\n",
       "      <td>17.908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>49.97</td>\n",
       "      <td>52.19</td>\n",
       "      <td>79</td>\n",
       "      <td>3.20</td>\n",
       "      <td>218011</td>\n",
       "      <td>17.909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>49.36</td>\n",
       "      <td>53.40</td>\n",
       "      <td>70</td>\n",
       "      <td>3.16</td>\n",
       "      <td>221815</td>\n",
       "      <td>17.470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>55.20</td>\n",
       "      <td>50.43</td>\n",
       "      <td>26</td>\n",
       "      <td>3.18</td>\n",
       "      <td>221965</td>\n",
       "      <td>17.343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>58.53</td>\n",
       "      <td>49.33</td>\n",
       "      <td>49</td>\n",
       "      <td>3.12</td>\n",
       "      <td>222025</td>\n",
       "      <td>17.072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>61.06</td>\n",
       "      <td>48.69</td>\n",
       "      <td>147</td>\n",
       "      <td>3.07</td>\n",
       "      <td>222398</td>\n",
       "      <td>17.160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>64.47</td>\n",
       "      <td>45.63</td>\n",
       "      <td>214</td>\n",
       "      <td>3.17</td>\n",
       "      <td>238347</td>\n",
       "      <td>17.079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>66.28</td>\n",
       "      <td>49.97</td>\n",
       "      <td>135</td>\n",
       "      <td>3.18</td>\n",
       "      <td>218978</td>\n",
       "      <td>17.211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>63.97</td>\n",
       "      <td>49.36</td>\n",
       "      <td>60</td>\n",
       "      <td>3.14</td>\n",
       "      <td>223602</td>\n",
       "      <td>17.199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>65.87</td>\n",
       "      <td>55.20</td>\n",
       "      <td>49</td>\n",
       "      <td>3.20</td>\n",
       "      <td>228198</td>\n",
       "      <td>17.234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>71.02</td>\n",
       "      <td>58.53</td>\n",
       "      <td>34</td>\n",
       "      <td>3.21</td>\n",
       "      <td>229757</td>\n",
       "      <td>17.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>75.18</td>\n",
       "      <td>61.06</td>\n",
       "      <td>83</td>\n",
       "      <td>3.32</td>\n",
       "      <td>229646</td>\n",
       "      <td>16.890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>75.68</td>\n",
       "      <td>64.47</td>\n",
       "      <td>26</td>\n",
       "      <td>3.47</td>\n",
       "      <td>239903</td>\n",
       "      <td>17.300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>73.62</td>\n",
       "      <td>66.28</td>\n",
       "      <td>23</td>\n",
       "      <td>3.52</td>\n",
       "      <td>225906</td>\n",
       "      <td>17.605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>75.74</td>\n",
       "      <td>63.97</td>\n",
       "      <td>16</td>\n",
       "      <td>3.65</td>\n",
       "      <td>234140</td>\n",
       "      <td>17.441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>81.48</td>\n",
       "      <td>65.87</td>\n",
       "      <td>59</td>\n",
       "      <td>3.66</td>\n",
       "      <td>242658</td>\n",
       "      <td>17.501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>75.24</td>\n",
       "      <td>71.02</td>\n",
       "      <td>49</td>\n",
       "      <td>3.74</td>\n",
       "      <td>239492</td>\n",
       "      <td>16.744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>58.33</td>\n",
       "      <td>75.18</td>\n",
       "      <td>154</td>\n",
       "      <td>3.80</td>\n",
       "      <td>240608</td>\n",
       "      <td>16.674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>51.55</td>\n",
       "      <td>75.68</td>\n",
       "      <td>188</td>\n",
       "      <td>3.87</td>\n",
       "      <td>243274</td>\n",
       "      <td>17.132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>61.19</td>\n",
       "      <td>73.62</td>\n",
       "      <td>59</td>\n",
       "      <td>3.87</td>\n",
       "      <td>234585</td>\n",
       "      <td>16.427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>65.28</td>\n",
       "      <td>75.74</td>\n",
       "      <td>95</td>\n",
       "      <td>3.85</td>\n",
       "      <td>244108</td>\n",
       "      <td>17.276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>67.23</td>\n",
       "      <td>81.48</td>\n",
       "      <td>76</td>\n",
       "      <td>3.97</td>\n",
       "      <td>244834</td>\n",
       "      <td>17.282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>71.91</td>\n",
       "      <td>75.24</td>\n",
       "      <td>59</td>\n",
       "      <td>4.14</td>\n",
       "      <td>239723</td>\n",
       "      <td>17.032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>64.15</td>\n",
       "      <td>58.33</td>\n",
       "      <td>156</td>\n",
       "      <td>4.12</td>\n",
       "      <td>231653</td>\n",
       "      <td>17.110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>65.60</td>\n",
       "      <td>51.55</td>\n",
       "      <td>92</td>\n",
       "      <td>4.00</td>\n",
       "      <td>238448</td>\n",
       "      <td>17.161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>65.53</td>\n",
       "      <td>61.19</td>\n",
       "      <td>223</td>\n",
       "      <td>3.96</td>\n",
       "      <td>229028</td>\n",
       "      <td>16.725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>60.11</td>\n",
       "      <td>65.28</td>\n",
       "      <td>54</td>\n",
       "      <td>3.84</td>\n",
       "      <td>223455</td>\n",
       "      <td>17.101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    oil_lag6  oil_lag12  ngf2_lag6  sfrm_lag12  durable_lag12  altsales_lag3\n",
       "0      50.51      44.83         22        2.86         224061         17.818\n",
       "1      47.01      56.83         21        2.99         218966         17.863\n",
       "2      43.58      51.06         62        2.92         227573         17.081\n",
       "3      43.66      62.21         64        2.85         224339         17.610\n",
       "4      38.93      60.47         27        2.90         217332         17.640\n",
       "5      31.27      58.79        103        2.98         232766         16.826\n",
       "6      29.11      50.51         56        2.95         230189         17.247\n",
       "7      30.13      47.01         37        2.90         222647         17.288\n",
       "8      34.33      43.58         57        2.91         218738         17.334\n",
       "9      42.70      43.66         82        2.89         226032         17.744\n",
       "10     45.15      38.93        180        3.01         223223         17.541\n",
       "11     46.27      31.27         30        3.08         217474         17.574\n",
       "12     38.97      29.11         38        2.90         226905         17.606\n",
       "13     43.91      30.13         79        2.79         217371         17.411\n",
       "14     44.63      34.33         75        2.90         216899         17.906\n",
       "15     45.83      42.70         84        2.86         228585         17.270\n",
       "16     44.80      45.15         28        2.87         221991         17.446\n",
       "17     53.30      46.27        193        2.70         209356         16.604\n",
       "18     52.19      38.97         44        2.78         217997         16.798\n",
       "19     53.40      43.91        263        2.75         219140         16.751\n",
       "20     50.43      44.63         27        2.81         216058         16.816\n",
       "21     49.33      45.83         40        2.84         234217         16.844\n",
       "22     48.69      44.80         71        3.12         216703         16.642\n",
       "23     45.63      53.30         57        3.30         218450         17.908\n",
       "24     49.97      52.19         79        3.20         218011         17.909\n",
       "25     49.36      53.40         70        3.16         221815         17.470\n",
       "26     55.20      50.43         26        3.18         221965         17.343\n",
       "27     58.53      49.33         49        3.12         222025         17.072\n",
       "28     61.06      48.69        147        3.07         222398         17.160\n",
       "29     64.47      45.63        214        3.17         238347         17.079\n",
       "30     66.28      49.97        135        3.18         218978         17.211\n",
       "31     63.97      49.36         60        3.14         223602         17.199\n",
       "32     65.87      55.20         49        3.20         228198         17.234\n",
       "33     71.02      58.53         34        3.21         229757         17.008\n",
       "34     75.18      61.06         83        3.32         229646         16.890\n",
       "35     75.68      64.47         26        3.47         239903         17.300\n",
       "36     73.62      66.28         23        3.52         225906         17.605\n",
       "37     75.74      63.97         16        3.65         234140         17.441\n",
       "38     81.48      65.87         59        3.66         242658         17.501\n",
       "39     75.24      71.02         49        3.74         239492         16.744\n",
       "40     58.33      75.18        154        3.80         240608         16.674\n",
       "41     51.55      75.68        188        3.87         243274         17.132\n",
       "42     61.19      73.62         59        3.87         234585         16.427\n",
       "43     65.28      75.74         95        3.85         244108         17.276\n",
       "44     67.23      81.48         76        3.97         244834         17.282\n",
       "45     71.91      75.24         59        4.14         239723         17.032\n",
       "46     64.15      58.33        156        4.12         231653         17.110\n",
       "47     65.60      51.55         92        4.00         238448         17.161\n",
       "48     65.53      61.19        223        3.96         229028         16.725\n",
       "49     60.11      65.28         54        3.84         223455         17.101"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = X.iloc[:, coef != 0]\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59e6b6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.insert(0, 'n', y, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8529538c",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.to_csv(\"data/out/lasso.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
